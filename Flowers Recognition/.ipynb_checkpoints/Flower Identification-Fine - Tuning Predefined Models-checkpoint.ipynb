{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications import vgg16, resnet50, mobilenet\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pred(preds, Y, val_breed, index, seq, ran):\n",
    "    leng = len(preds)\n",
    "    if seq:\n",
    "        for i in range(index):\n",
    "            if ran:\n",
    "                index = random.randint(0, leng) \n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "    else:\n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "        \n",
    "def accuracy_func(preds, Y, val_breed):\n",
    "    leng = len(preds)\n",
    "    count = 0;\n",
    "    for i in range(leng):\n",
    "        _, imagenet_class_name, prob = decode_predictions(preds, top=1)[i][0]\n",
    "        if val_breed[Y[i]] == imagenet_class_name:\n",
    "            count+=1\n",
    "    accuracy = (count/leng)*100\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "vgg_model = vgg16.VGG16(weights='imagenet')\n",
    " \n",
    "#Load the Inception_V3 model\n",
    "inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "#Load the ResNet50 model\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "#Load the MobileNet model\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = r'data/train'\n",
    "validation_path = r'data/validation'\n",
    "testing_path = r'data/test'\n",
    "batch_size = 32\n",
    "target_size=(224, 224)\n",
    "norm = 255.0\n",
    "class_mode='categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3031 images belonging to 5 classes.\n",
      "Found 866 images belonging to 5 classes.\n",
      "Found 426 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./norm,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "test_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testing_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(5, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_5\n",
      "1 conv2d_95\n",
      "2 batch_normalization_95\n",
      "3 activation_144\n",
      "4 conv2d_96\n",
      "5 batch_normalization_96\n",
      "6 activation_145\n",
      "7 conv2d_97\n",
      "8 batch_normalization_97\n",
      "9 activation_146\n",
      "10 max_pooling2d_6\n",
      "11 conv2d_98\n",
      "12 batch_normalization_98\n",
      "13 activation_147\n",
      "14 conv2d_99\n",
      "15 batch_normalization_99\n",
      "16 activation_148\n",
      "17 max_pooling2d_7\n",
      "18 conv2d_103\n",
      "19 batch_normalization_103\n",
      "20 activation_152\n",
      "21 conv2d_101\n",
      "22 conv2d_104\n",
      "23 batch_normalization_101\n",
      "24 batch_normalization_104\n",
      "25 activation_150\n",
      "26 activation_153\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_100\n",
      "29 conv2d_102\n",
      "30 conv2d_105\n",
      "31 conv2d_106\n",
      "32 batch_normalization_100\n",
      "33 batch_normalization_102\n",
      "34 batch_normalization_105\n",
      "35 batch_normalization_106\n",
      "36 activation_149\n",
      "37 activation_151\n",
      "38 activation_154\n",
      "39 activation_155\n",
      "40 mixed0\n",
      "41 conv2d_110\n",
      "42 batch_normalization_110\n",
      "43 activation_159\n",
      "44 conv2d_108\n",
      "45 conv2d_111\n",
      "46 batch_normalization_108\n",
      "47 batch_normalization_111\n",
      "48 activation_157\n",
      "49 activation_160\n",
      "50 average_pooling2d_11\n",
      "51 conv2d_107\n",
      "52 conv2d_109\n",
      "53 conv2d_112\n",
      "54 conv2d_113\n",
      "55 batch_normalization_107\n",
      "56 batch_normalization_109\n",
      "57 batch_normalization_112\n",
      "58 batch_normalization_113\n",
      "59 activation_156\n",
      "60 activation_158\n",
      "61 activation_161\n",
      "62 activation_162\n",
      "63 mixed1\n",
      "64 conv2d_117\n",
      "65 batch_normalization_117\n",
      "66 activation_166\n",
      "67 conv2d_115\n",
      "68 conv2d_118\n",
      "69 batch_normalization_115\n",
      "70 batch_normalization_118\n",
      "71 activation_164\n",
      "72 activation_167\n",
      "73 average_pooling2d_12\n",
      "74 conv2d_114\n",
      "75 conv2d_116\n",
      "76 conv2d_119\n",
      "77 conv2d_120\n",
      "78 batch_normalization_114\n",
      "79 batch_normalization_116\n",
      "80 batch_normalization_119\n",
      "81 batch_normalization_120\n",
      "82 activation_163\n",
      "83 activation_165\n",
      "84 activation_168\n",
      "85 activation_169\n",
      "86 mixed2\n",
      "87 conv2d_122\n",
      "88 batch_normalization_122\n",
      "89 activation_171\n",
      "90 conv2d_123\n",
      "91 batch_normalization_123\n",
      "92 activation_172\n",
      "93 conv2d_121\n",
      "94 conv2d_124\n",
      "95 batch_normalization_121\n",
      "96 batch_normalization_124\n",
      "97 activation_170\n",
      "98 activation_173\n",
      "99 max_pooling2d_8\n",
      "100 mixed3\n",
      "101 conv2d_129\n",
      "102 batch_normalization_129\n",
      "103 activation_178\n",
      "104 conv2d_130\n",
      "105 batch_normalization_130\n",
      "106 activation_179\n",
      "107 conv2d_126\n",
      "108 conv2d_131\n",
      "109 batch_normalization_126\n",
      "110 batch_normalization_131\n",
      "111 activation_175\n",
      "112 activation_180\n",
      "113 conv2d_127\n",
      "114 conv2d_132\n",
      "115 batch_normalization_127\n",
      "116 batch_normalization_132\n",
      "117 activation_176\n",
      "118 activation_181\n",
      "119 average_pooling2d_13\n",
      "120 conv2d_125\n",
      "121 conv2d_128\n",
      "122 conv2d_133\n",
      "123 conv2d_134\n",
      "124 batch_normalization_125\n",
      "125 batch_normalization_128\n",
      "126 batch_normalization_133\n",
      "127 batch_normalization_134\n",
      "128 activation_174\n",
      "129 activation_177\n",
      "130 activation_182\n",
      "131 activation_183\n",
      "132 mixed4\n",
      "133 conv2d_139\n",
      "134 batch_normalization_139\n",
      "135 activation_188\n",
      "136 conv2d_140\n",
      "137 batch_normalization_140\n",
      "138 activation_189\n",
      "139 conv2d_136\n",
      "140 conv2d_141\n",
      "141 batch_normalization_136\n",
      "142 batch_normalization_141\n",
      "143 activation_185\n",
      "144 activation_190\n",
      "145 conv2d_137\n",
      "146 conv2d_142\n",
      "147 batch_normalization_137\n",
      "148 batch_normalization_142\n",
      "149 activation_186\n",
      "150 activation_191\n",
      "151 average_pooling2d_14\n",
      "152 conv2d_135\n",
      "153 conv2d_138\n",
      "154 conv2d_143\n",
      "155 conv2d_144\n",
      "156 batch_normalization_135\n",
      "157 batch_normalization_138\n",
      "158 batch_normalization_143\n",
      "159 batch_normalization_144\n",
      "160 activation_184\n",
      "161 activation_187\n",
      "162 activation_192\n",
      "163 activation_193\n",
      "164 mixed5\n",
      "165 conv2d_149\n",
      "166 batch_normalization_149\n",
      "167 activation_198\n",
      "168 conv2d_150\n",
      "169 batch_normalization_150\n",
      "170 activation_199\n",
      "171 conv2d_146\n",
      "172 conv2d_151\n",
      "173 batch_normalization_146\n",
      "174 batch_normalization_151\n",
      "175 activation_195\n",
      "176 activation_200\n",
      "177 conv2d_147\n",
      "178 conv2d_152\n",
      "179 batch_normalization_147\n",
      "180 batch_normalization_152\n",
      "181 activation_196\n",
      "182 activation_201\n",
      "183 average_pooling2d_15\n",
      "184 conv2d_145\n",
      "185 conv2d_148\n",
      "186 conv2d_153\n",
      "187 conv2d_154\n",
      "188 batch_normalization_145\n",
      "189 batch_normalization_148\n",
      "190 batch_normalization_153\n",
      "191 batch_normalization_154\n",
      "192 activation_194\n",
      "193 activation_197\n",
      "194 activation_202\n",
      "195 activation_203\n",
      "196 mixed6\n",
      "197 conv2d_159\n",
      "198 batch_normalization_159\n",
      "199 activation_208\n",
      "200 conv2d_160\n",
      "201 batch_normalization_160\n",
      "202 activation_209\n",
      "203 conv2d_156\n",
      "204 conv2d_161\n",
      "205 batch_normalization_156\n",
      "206 batch_normalization_161\n",
      "207 activation_205\n",
      "208 activation_210\n",
      "209 conv2d_157\n",
      "210 conv2d_162\n",
      "211 batch_normalization_157\n",
      "212 batch_normalization_162\n",
      "213 activation_206\n",
      "214 activation_211\n",
      "215 average_pooling2d_16\n",
      "216 conv2d_155\n",
      "217 conv2d_158\n",
      "218 conv2d_163\n",
      "219 conv2d_164\n",
      "220 batch_normalization_155\n",
      "221 batch_normalization_158\n",
      "222 batch_normalization_163\n",
      "223 batch_normalization_164\n",
      "224 activation_204\n",
      "225 activation_207\n",
      "226 activation_212\n",
      "227 activation_213\n",
      "228 mixed7\n",
      "229 conv2d_167\n",
      "230 batch_normalization_167\n",
      "231 activation_216\n",
      "232 conv2d_168\n",
      "233 batch_normalization_168\n",
      "234 activation_217\n",
      "235 conv2d_165\n",
      "236 conv2d_169\n",
      "237 batch_normalization_165\n",
      "238 batch_normalization_169\n",
      "239 activation_214\n",
      "240 activation_218\n",
      "241 conv2d_166\n",
      "242 conv2d_170\n",
      "243 batch_normalization_166\n",
      "244 batch_normalization_170\n",
      "245 activation_215\n",
      "246 activation_219\n",
      "247 max_pooling2d_9\n",
      "248 mixed8\n",
      "249 conv2d_175\n",
      "250 batch_normalization_175\n",
      "251 activation_224\n",
      "252 conv2d_172\n",
      "253 conv2d_176\n",
      "254 batch_normalization_172\n",
      "255 batch_normalization_176\n",
      "256 activation_221\n",
      "257 activation_225\n",
      "258 conv2d_173\n",
      "259 conv2d_174\n",
      "260 conv2d_177\n",
      "261 conv2d_178\n",
      "262 average_pooling2d_17\n",
      "263 conv2d_171\n",
      "264 batch_normalization_173\n",
      "265 batch_normalization_174\n",
      "266 batch_normalization_177\n",
      "267 batch_normalization_178\n",
      "268 conv2d_179\n",
      "269 batch_normalization_171\n",
      "270 activation_222\n",
      "271 activation_223\n",
      "272 activation_226\n",
      "273 activation_227\n",
      "274 batch_normalization_179\n",
      "275 activation_220\n",
      "276 mixed9_0\n",
      "277 concatenate_3\n",
      "278 activation_228\n",
      "279 mixed9\n",
      "280 conv2d_184\n",
      "281 batch_normalization_184\n",
      "282 activation_233\n",
      "283 conv2d_181\n",
      "284 conv2d_185\n",
      "285 batch_normalization_181\n",
      "286 batch_normalization_185\n",
      "287 activation_230\n",
      "288 activation_234\n",
      "289 conv2d_182\n",
      "290 conv2d_183\n",
      "291 conv2d_186\n",
      "292 conv2d_187\n",
      "293 average_pooling2d_18\n",
      "294 conv2d_180\n",
      "295 batch_normalization_182\n",
      "296 batch_normalization_183\n",
      "297 batch_normalization_186\n",
      "298 batch_normalization_187\n",
      "299 conv2d_188\n",
      "300 batch_normalization_180\n",
      "301 activation_231\n",
      "302 activation_232\n",
      "303 activation_235\n",
      "304 activation_236\n",
      "305 batch_normalization_188\n",
      "306 activation_229\n",
      "307 mixed9_1\n",
      "308 concatenate_4\n",
      "309 activation_237\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'output/models/'\n",
    "log_file = \"output/logs\"\n",
    "\n",
    "model_file = model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_file, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "tensorboard.set_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "95/95 [==============================] - ETA: 1:08:37 - loss: 1.8640 - acc: 0.03 - ETA: 43:03 - loss: 1.8273 - acc: 0.1250 - ETA: 35:03 - loss: 1.8006 - acc: 0.12 - ETA: 30:18 - loss: 1.7888 - acc: 0.14 - ETA: 28:05 - loss: 1.7822 - acc: 0.15 - ETA: 26:26 - loss: 1.7959 - acc: 0.14 - ETA: 24:58 - loss: 1.7882 - acc: 0.13 - ETA: 23:53 - loss: 1.7944 - acc: 0.13 - ETA: 22:56 - loss: 1.7888 - acc: 0.13 - ETA: 22:22 - loss: 1.7767 - acc: 0.14 - ETA: 21:51 - loss: 1.7711 - acc: 0.13 - ETA: 21:28 - loss: 1.7735 - acc: 0.14 - ETA: 20:59 - loss: 1.7685 - acc: 0.14 - ETA: 20:20 - loss: 1.7632 - acc: 0.14 - ETA: 19:56 - loss: 1.7558 - acc: 0.15 - ETA: 19:33 - loss: 1.7568 - acc: 0.15 - ETA: 19:11 - loss: 1.7481 - acc: 0.16 - ETA: 18:44 - loss: 1.7526 - acc: 0.16 - ETA: 18:19 - loss: 1.7505 - acc: 0.16 - ETA: 17:58 - loss: 1.7453 - acc: 0.16 - ETA: 17:44 - loss: 1.7393 - acc: 0.17 - ETA: 17:21 - loss: 1.7361 - acc: 0.17 - ETA: 17:01 - loss: 1.7326 - acc: 0.17 - ETA: 16:40 - loss: 1.7186 - acc: 0.19 - ETA: 16:25 - loss: 1.7101 - acc: 0.20 - ETA: 16:11 - loss: 1.7068 - acc: 0.20 - ETA: 15:54 - loss: 1.7063 - acc: 0.19 - ETA: 15:34 - loss: 1.7041 - acc: 0.20 - ETA: 15:19 - loss: 1.6998 - acc: 0.20 - ETA: 15:03 - loss: 1.6969 - acc: 0.20 - ETA: 14:44 - loss: 1.6903 - acc: 0.20 - ETA: 14:26 - loss: 1.6897 - acc: 0.20 - ETA: 14:15 - loss: 1.6840 - acc: 0.21 - ETA: 13:58 - loss: 1.6798 - acc: 0.21 - ETA: 13:42 - loss: 1.6789 - acc: 0.21 - ETA: 13:30 - loss: 1.6744 - acc: 0.22 - ETA: 13:13 - loss: 1.6728 - acc: 0.22 - ETA: 12:58 - loss: 1.6687 - acc: 0.22 - ETA: 12:43 - loss: 1.6665 - acc: 0.22 - ETA: 12:27 - loss: 1.6624 - acc: 0.23 - ETA: 12:06 - loss: 1.6630 - acc: 0.23 - ETA: 11:49 - loss: 1.6613 - acc: 0.23 - ETA: 11:34 - loss: 1.6593 - acc: 0.23 - ETA: 11:18 - loss: 1.6570 - acc: 0.24 - ETA: 11:03 - loss: 1.6542 - acc: 0.24 - ETA: 10:49 - loss: 1.6504 - acc: 0.24 - ETA: 10:37 - loss: 1.6460 - acc: 0.25 - ETA: 10:22 - loss: 1.6436 - acc: 0.25 - ETA: 10:08 - loss: 1.6404 - acc: 0.25 - ETA: 9:55 - loss: 1.6367 - acc: 0.2576 - ETA: 9:42 - loss: 1.6349 - acc: 0.258 - ETA: 9:29 - loss: 1.6323 - acc: 0.260 - ETA: 9:14 - loss: 1.6304 - acc: 0.261 - ETA: 8:59 - loss: 1.6282 - acc: 0.263 - ETA: 8:45 - loss: 1.6273 - acc: 0.264 - ETA: 8:31 - loss: 1.6237 - acc: 0.269 - ETA: 8:16 - loss: 1.6232 - acc: 0.270 - ETA: 8:03 - loss: 1.6211 - acc: 0.273 - ETA: 7:49 - loss: 1.6198 - acc: 0.275 - ETA: 7:35 - loss: 1.6186 - acc: 0.275 - ETA: 7:22 - loss: 1.6177 - acc: 0.276 - ETA: 7:08 - loss: 1.6157 - acc: 0.276 - ETA: 6:54 - loss: 1.6146 - acc: 0.277 - ETA: 6:41 - loss: 1.6129 - acc: 0.277 - ETA: 6:27 - loss: 1.6105 - acc: 0.282 - ETA: 6:14 - loss: 1.6078 - acc: 0.285 - ETA: 6:00 - loss: 1.6062 - acc: 0.285 - ETA: 5:46 - loss: 1.6027 - acc: 0.290 - ETA: 5:33 - loss: 1.6006 - acc: 0.292 - ETA: 5:19 - loss: 1.5977 - acc: 0.295 - ETA: 5:05 - loss: 1.5955 - acc: 0.298 - ETA: 4:52 - loss: 1.5931 - acc: 0.300 - ETA: 4:38 - loss: 1.5916 - acc: 0.300 - ETA: 4:25 - loss: 1.5905 - acc: 0.300 - ETA: 4:12 - loss: 1.5885 - acc: 0.301 - ETA: 3:59 - loss: 1.5859 - acc: 0.303 - ETA: 3:45 - loss: 1.5842 - acc: 0.305 - ETA: 3:32 - loss: 1.5824 - acc: 0.307 - ETA: 3:19 - loss: 1.5801 - acc: 0.309 - ETA: 3:06 - loss: 1.5780 - acc: 0.312 - ETA: 2:54 - loss: 1.5764 - acc: 0.313 - ETA: 2:41 - loss: 1.5752 - acc: 0.314 - ETA: 2:28 - loss: 1.5732 - acc: 0.317 - ETA: 2:16 - loss: 1.5705 - acc: 0.320 - ETA: 2:03 - loss: 1.5686 - acc: 0.323 - ETA: 1:50 - loss: 1.5665 - acc: 0.326 - ETA: 1:38 - loss: 1.5648 - acc: 0.326 - ETA: 1:25 - loss: 1.5633 - acc: 0.327 - ETA: 1:13 - loss: 1.5605 - acc: 0.330 - ETA: 1:01 - loss: 1.5592 - acc: 0.331 - ETA: 49s - loss: 1.5568 - acc: 0.333 - ETA: 36s - loss: 1.5545 - acc: 0.33 - ETA: 24s - loss: 1.5532 - acc: 0.33 - ETA: 12s - loss: 1.5512 - acc: 0.33 - 1501s 16s/step - loss: 1.5490 - acc: 0.3408 - val_loss: 1.3747 - val_acc: 0.4850\n",
      "Epoch 2/100\n",
      "95/95 [==============================] - ETA: 17:56 - loss: 1.4568 - acc: 0.43 - ETA: 18:21 - loss: 1.3947 - acc: 0.48 - ETA: 17:43 - loss: 1.3751 - acc: 0.48 - ETA: 17:18 - loss: 1.3792 - acc: 0.49 - ETA: 16:46 - loss: 1.3600 - acc: 0.50 - ETA: 16:34 - loss: 1.3713 - acc: 0.48 - ETA: 16:39 - loss: 1.3569 - acc: 0.51 - ETA: 16:32 - loss: 1.3539 - acc: 0.52 - ETA: 16:32 - loss: 1.3571 - acc: 0.52 - ETA: 16:18 - loss: 1.3759 - acc: 0.50 - ETA: 16:08 - loss: 1.3795 - acc: 0.50 - ETA: 16:06 - loss: 1.3707 - acc: 0.51 - ETA: 15:49 - loss: 1.3676 - acc: 0.52 - ETA: 15:41 - loss: 1.3777 - acc: 0.50 - ETA: 15:28 - loss: 1.3815 - acc: 0.49 - ETA: 15:13 - loss: 1.3726 - acc: 0.50 - ETA: 14:58 - loss: 1.3688 - acc: 0.50 - ETA: 14:57 - loss: 1.3684 - acc: 0.50 - ETA: 14:47 - loss: 1.3667 - acc: 0.50 - ETA: 14:32 - loss: 1.3614 - acc: 0.50 - ETA: 14:17 - loss: 1.3568 - acc: 0.50 - ETA: 14:01 - loss: 1.3544 - acc: 0.51 - ETA: 13:45 - loss: 1.3545 - acc: 0.51 - ETA: 13:31 - loss: 1.3528 - acc: 0.51 - ETA: 13:18 - loss: 1.3519 - acc: 0.52 - ETA: 13:04 - loss: 1.3517 - acc: 0.52 - ETA: 12:51 - loss: 1.3519 - acc: 0.51 - ETA: 12:39 - loss: 1.3475 - acc: 0.52 - ETA: 12:26 - loss: 1.3430 - acc: 0.52 - ETA: 12:14 - loss: 1.3454 - acc: 0.52 - ETA: 12:02 - loss: 1.3426 - acc: 0.53 - ETA: 11:50 - loss: 1.3424 - acc: 0.53 - ETA: 11:38 - loss: 1.3394 - acc: 0.53 - ETA: 11:26 - loss: 1.3383 - acc: 0.53 - ETA: 11:14 - loss: 1.3377 - acc: 0.53 - ETA: 11:02 - loss: 1.3362 - acc: 0.53 - ETA: 10:51 - loss: 1.3357 - acc: 0.54 - ETA: 10:40 - loss: 1.3354 - acc: 0.54 - ETA: 10:28 - loss: 1.3335 - acc: 0.54 - ETA: 10:17 - loss: 1.3337 - acc: 0.54 - ETA: 10:06 - loss: 1.3351 - acc: 0.54 - ETA: 9:54 - loss: 1.3337 - acc: 0.5476 - ETA: 9:43 - loss: 1.3321 - acc: 0.550 - ETA: 9:32 - loss: 1.3314 - acc: 0.551 - ETA: 9:21 - loss: 1.3302 - acc: 0.552 - ETA: 9:09 - loss: 1.3288 - acc: 0.551 - ETA: 8:58 - loss: 1.3273 - acc: 0.552 - ETA: 8:46 - loss: 1.3262 - acc: 0.554 - ETA: 8:35 - loss: 1.3249 - acc: 0.556 - ETA: 8:24 - loss: 1.3243 - acc: 0.558 - ETA: 8:13 - loss: 1.3230 - acc: 0.557 - ETA: 8:02 - loss: 1.3223 - acc: 0.557 - ETA: 7:51 - loss: 1.3205 - acc: 0.559 - ETA: 7:41 - loss: 1.3182 - acc: 0.561 - ETA: 7:30 - loss: 1.3174 - acc: 0.562 - ETA: 7:19 - loss: 1.3164 - acc: 0.561 - ETA: 7:08 - loss: 1.3150 - acc: 0.564 - ETA: 6:56 - loss: 1.3139 - acc: 0.564 - ETA: 6:45 - loss: 1.3125 - acc: 0.566 - ETA: 6:34 - loss: 1.3096 - acc: 0.567 - ETA: 6:24 - loss: 1.3083 - acc: 0.569 - ETA: 6:12 - loss: 1.3070 - acc: 0.569 - ETA: 6:01 - loss: 1.3044 - acc: 0.570 - ETA: 5:50 - loss: 1.3012 - acc: 0.573 - ETA: 5:38 - loss: 1.3006 - acc: 0.573 - ETA: 5:27 - loss: 1.2999 - acc: 0.574 - ETA: 5:15 - loss: 1.2987 - acc: 0.575 - ETA: 5:03 - loss: 1.2969 - acc: 0.578 - ETA: 4:52 - loss: 1.2956 - acc: 0.578 - ETA: 4:41 - loss: 1.2954 - acc: 0.578 - ETA: 4:29 - loss: 1.2947 - acc: 0.579 - ETA: 4:18 - loss: 1.2929 - acc: 0.580 - ETA: 4:07 - loss: 1.2917 - acc: 0.581 - ETA: 3:55 - loss: 1.2899 - acc: 0.582 - ETA: 3:44 - loss: 1.2870 - acc: 0.584 - ETA: 3:33 - loss: 1.2863 - acc: 0.585 - ETA: 3:21 - loss: 1.2854 - acc: 0.586 - ETA: 3:10 - loss: 1.2843 - acc: 0.587 - ETA: 2:59 - loss: 1.2813 - acc: 0.590 - ETA: 2:48 - loss: 1.2819 - acc: 0.589 - ETA: 2:37 - loss: 1.2822 - acc: 0.588 - ETA: 2:25 - loss: 1.2807 - acc: 0.590 - ETA: 2:14 - loss: 1.2801 - acc: 0.591 - ETA: 2:03 - loss: 1.2783 - acc: 0.591 - ETA: 1:52 - loss: 1.2768 - acc: 0.593 - ETA: 1:40 - loss: 1.2756 - acc: 0.594 - ETA: 1:29 - loss: 1.2747 - acc: 0.594 - ETA: 1:18 - loss: 1.2727 - acc: 0.595 - ETA: 1:07 - loss: 1.2707 - acc: 0.597 - ETA: 56s - loss: 1.2694 - acc: 0.597 - ETA: 44s - loss: 1.2690 - acc: 0.59 - ETA: 33s - loss: 1.2672 - acc: 0.59 - ETA: 22s - loss: 1.2654 - acc: 0.60 - ETA: 11s - loss: 1.2655 - acc: 0.60 - 1420s 15s/step - loss: 1.2643 - acc: 0.6019 - val_loss: 1.1294 - val_acc: 0.6236\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 16:13 - loss: 1.0585 - acc: 0.87 - ETA: 16:11 - loss: 1.1163 - acc: 0.76 - ETA: 15:56 - loss: 1.0945 - acc: 0.76 - ETA: 16:44 - loss: 1.1294 - acc: 0.73 - ETA: 17:12 - loss: 1.1192 - acc: 0.71 - ETA: 17:00 - loss: 1.1203 - acc: 0.71 - ETA: 16:43 - loss: 1.1072 - acc: 0.71 - ETA: 16:38 - loss: 1.1152 - acc: 0.69 - ETA: 16:21 - loss: 1.1160 - acc: 0.69 - ETA: 16:04 - loss: 1.1105 - acc: 0.69 - ETA: 15:53 - loss: 1.1011 - acc: 0.69 - ETA: 15:53 - loss: 1.1077 - acc: 0.69 - ETA: 15:42 - loss: 1.1088 - acc: 0.68 - ETA: 15:32 - loss: 1.1084 - acc: 0.69 - ETA: 15:16 - loss: 1.1102 - acc: 0.69 - ETA: 15:08 - loss: 1.1135 - acc: 0.68 - ETA: 14:57 - loss: 1.1222 - acc: 0.68 - ETA: 14:43 - loss: 1.1204 - acc: 0.68 - ETA: 14:29 - loss: 1.1290 - acc: 0.66 - ETA: 14:17 - loss: 1.1288 - acc: 0.66 - ETA: 14:09 - loss: 1.1312 - acc: 0.66 - ETA: 13:58 - loss: 1.1320 - acc: 0.66 - ETA: 13:48 - loss: 1.1326 - acc: 0.66 - ETA: 13:37 - loss: 1.1348 - acc: 0.66 - ETA: 13:23 - loss: 1.1358 - acc: 0.66 - ETA: 13:11 - loss: 1.1339 - acc: 0.66 - ETA: 12:58 - loss: 1.1290 - acc: 0.67 - ETA: 12:44 - loss: 1.1283 - acc: 0.67 - ETA: 12:29 - loss: 1.1258 - acc: 0.67 - ETA: 12:15 - loss: 1.1275 - acc: 0.67 - ETA: 12:01 - loss: 1.1245 - acc: 0.67 - ETA: 11:49 - loss: 1.1244 - acc: 0.67 - ETA: 11:36 - loss: 1.1209 - acc: 0.67 - ETA: 11:24 - loss: 1.1223 - acc: 0.67 - ETA: 11:11 - loss: 1.1226 - acc: 0.67 - ETA: 10:59 - loss: 1.1184 - acc: 0.67 - ETA: 10:48 - loss: 1.1172 - acc: 0.67 - ETA: 10:37 - loss: 1.1138 - acc: 0.67 - ETA: 10:26 - loss: 1.1115 - acc: 0.67 - ETA: 10:16 - loss: 1.1113 - acc: 0.67 - ETA: 10:05 - loss: 1.1094 - acc: 0.68 - ETA: 9:54 - loss: 1.1095 - acc: 0.6815 - ETA: 9:43 - loss: 1.1058 - acc: 0.685 - ETA: 9:33 - loss: 1.1066 - acc: 0.684 - ETA: 9:22 - loss: 1.1064 - acc: 0.684 - ETA: 9:14 - loss: 1.1053 - acc: 0.686 - ETA: 9:05 - loss: 1.1054 - acc: 0.684 - ETA: 8:54 - loss: 1.1039 - acc: 0.684 - ETA: 8:42 - loss: 1.1020 - acc: 0.686 - ETA: 8:30 - loss: 1.0993 - acc: 0.690 - ETA: 8:19 - loss: 1.0996 - acc: 0.689 - ETA: 8:08 - loss: 1.0997 - acc: 0.688 - ETA: 7:57 - loss: 1.0987 - acc: 0.689 - ETA: 7:46 - loss: 1.0982 - acc: 0.688 - ETA: 7:36 - loss: 1.0966 - acc: 0.690 - ETA: 7:25 - loss: 1.0961 - acc: 0.690 - ETA: 7:15 - loss: 1.0953 - acc: 0.692 - ETA: 7:04 - loss: 1.0932 - acc: 0.692 - ETA: 6:53 - loss: 1.0936 - acc: 0.691 - ETA: 6:43 - loss: 1.0934 - acc: 0.691 - ETA: 6:31 - loss: 1.0911 - acc: 0.693 - ETA: 6:19 - loss: 1.0899 - acc: 0.693 - ETA: 6:07 - loss: 1.0872 - acc: 0.694 - ETA: 5:55 - loss: 1.0859 - acc: 0.696 - ETA: 5:44 - loss: 1.0848 - acc: 0.696 - ETA: 5:33 - loss: 1.0852 - acc: 0.696 - ETA: 5:22 - loss: 1.0845 - acc: 0.695 - ETA: 5:10 - loss: 1.0827 - acc: 0.696 - ETA: 4:59 - loss: 1.0818 - acc: 0.697 - ETA: 4:47 - loss: 1.0830 - acc: 0.694 - ETA: 4:36 - loss: 1.0814 - acc: 0.696 - ETA: 4:24 - loss: 1.0801 - acc: 0.696 - ETA: 4:12 - loss: 1.0797 - acc: 0.696 - ETA: 4:00 - loss: 1.0779 - acc: 0.697 - ETA: 3:49 - loss: 1.0776 - acc: 0.697 - ETA: 3:37 - loss: 1.0770 - acc: 0.697 - ETA: 3:26 - loss: 1.0744 - acc: 0.699 - ETA: 3:14 - loss: 1.0739 - acc: 0.699 - ETA: 3:03 - loss: 1.0724 - acc: 0.701 - ETA: 2:51 - loss: 1.0712 - acc: 0.702 - ETA: 2:40 - loss: 1.0721 - acc: 0.702 - ETA: 2:28 - loss: 1.0728 - acc: 0.700 - ETA: 2:17 - loss: 1.0727 - acc: 0.701 - ETA: 2:05 - loss: 1.0717 - acc: 0.701 - ETA: 1:54 - loss: 1.0702 - acc: 0.701 - ETA: 1:42 - loss: 1.0705 - acc: 0.701 - ETA: 1:31 - loss: 1.0697 - acc: 0.702 - ETA: 1:19 - loss: 1.0678 - acc: 0.703 - ETA: 1:08 - loss: 1.0673 - acc: 0.703 - ETA: 57s - loss: 1.0647 - acc: 0.704 - ETA: 45s - loss: 1.0638 - acc: 0.70 - ETA: 34s - loss: 1.0618 - acc: 0.70 - ETA: 22s - loss: 1.0607 - acc: 0.70 - ETA: 11s - loss: 1.0597 - acc: 0.70 - 1448s 15s/step - loss: 1.0596 - acc: 0.7076 - val_loss: 0.9776 - val_acc: 0.6767\n",
      "Epoch 4/100\n",
      "95/95 [==============================] - ETA: 20:35 - loss: 0.9738 - acc: 0.75 - ETA: 18:58 - loss: 0.9536 - acc: 0.76 - ETA: 18:40 - loss: 0.9611 - acc: 0.79 - ETA: 18:52 - loss: 0.9574 - acc: 0.78 - ETA: 18:14 - loss: 0.9610 - acc: 0.76 - ETA: 17:49 - loss: 0.9857 - acc: 0.72 - ETA: 17:49 - loss: 0.9781 - acc: 0.73 - ETA: 17:40 - loss: 0.9580 - acc: 0.75 - ETA: 17:23 - loss: 0.9702 - acc: 0.73 - ETA: 17:24 - loss: 0.9955 - acc: 0.70 - ETA: 17:23 - loss: 0.9922 - acc: 0.71 - ETA: 17:06 - loss: 1.0118 - acc: 0.69 - ETA: 17:09 - loss: 1.0047 - acc: 0.70 - ETA: 16:56 - loss: 1.0100 - acc: 0.70 - ETA: 16:41 - loss: 1.0067 - acc: 0.70 - ETA: 16:25 - loss: 1.0064 - acc: 0.70 - ETA: 16:04 - loss: 1.0043 - acc: 0.69 - ETA: 15:49 - loss: 1.0006 - acc: 0.70 - ETA: 15:30 - loss: 0.9958 - acc: 0.70 - ETA: 15:13 - loss: 0.9884 - acc: 0.70 - ETA: 14:57 - loss: 0.9906 - acc: 0.70 - ETA: 14:40 - loss: 0.9862 - acc: 0.70 - ETA: 14:28 - loss: 0.9821 - acc: 0.71 - ETA: 14:13 - loss: 0.9851 - acc: 0.70 - ETA: 14:01 - loss: 0.9861 - acc: 0.70 - ETA: 13:37 - loss: 0.9867 - acc: 0.70 - ETA: 13:25 - loss: 0.9842 - acc: 0.70 - ETA: 13:14 - loss: 0.9798 - acc: 0.71 - ETA: 13:02 - loss: 0.9806 - acc: 0.71 - ETA: 12:48 - loss: 0.9812 - acc: 0.71 - ETA: 12:35 - loss: 0.9807 - acc: 0.71 - ETA: 12:21 - loss: 0.9811 - acc: 0.70 - ETA: 12:08 - loss: 0.9848 - acc: 0.70 - ETA: 11:58 - loss: 0.9826 - acc: 0.70 - ETA: 11:45 - loss: 0.9781 - acc: 0.71 - ETA: 11:33 - loss: 0.9809 - acc: 0.71 - ETA: 11:22 - loss: 0.9784 - acc: 0.71 - ETA: 11:09 - loss: 0.9796 - acc: 0.70 - ETA: 10:56 - loss: 0.9784 - acc: 0.71 - ETA: 10:43 - loss: 0.9763 - acc: 0.71 - ETA: 10:30 - loss: 0.9751 - acc: 0.71 - ETA: 10:19 - loss: 0.9747 - acc: 0.71 - ETA: 10:07 - loss: 0.9756 - acc: 0.71 - ETA: 9:54 - loss: 0.9733 - acc: 0.7154 - ETA: 9:41 - loss: 0.9716 - acc: 0.716 - ETA: 9:29 - loss: 0.9696 - acc: 0.718 - ETA: 9:16 - loss: 0.9680 - acc: 0.719 - ETA: 9:03 - loss: 0.9677 - acc: 0.720 - ETA: 8:52 - loss: 0.9678 - acc: 0.720 - ETA: 8:41 - loss: 0.9686 - acc: 0.720 - ETA: 8:29 - loss: 0.9643 - acc: 0.724 - ETA: 8:16 - loss: 0.9607 - acc: 0.726 - ETA: 8:04 - loss: 0.9602 - acc: 0.726 - ETA: 7:53 - loss: 0.9619 - acc: 0.724 - ETA: 7:41 - loss: 0.9584 - acc: 0.727 - ETA: 7:30 - loss: 0.9603 - acc: 0.727 - ETA: 7:18 - loss: 0.9588 - acc: 0.728 - ETA: 7:07 - loss: 0.9570 - acc: 0.731 - ETA: 6:55 - loss: 0.9563 - acc: 0.732 - ETA: 6:43 - loss: 0.9560 - acc: 0.732 - ETA: 6:31 - loss: 0.9564 - acc: 0.733 - ETA: 6:19 - loss: 0.9556 - acc: 0.732 - ETA: 6:07 - loss: 0.9532 - acc: 0.734 - ETA: 5:56 - loss: 0.9536 - acc: 0.734 - ETA: 5:44 - loss: 0.9510 - acc: 0.736 - ETA: 5:32 - loss: 0.9506 - acc: 0.736 - ETA: 5:21 - loss: 0.9505 - acc: 0.737 - ETA: 5:09 - loss: 0.9484 - acc: 0.735 - ETA: 4:58 - loss: 0.9489 - acc: 0.734 - ETA: 4:47 - loss: 0.9498 - acc: 0.733 - ETA: 4:35 - loss: 0.9482 - acc: 0.735 - ETA: 4:24 - loss: 0.9459 - acc: 0.736 - ETA: 4:12 - loss: 0.9447 - acc: 0.736 - ETA: 4:01 - loss: 0.9461 - acc: 0.734 - ETA: 3:49 - loss: 0.9429 - acc: 0.737 - ETA: 3:38 - loss: 0.9416 - acc: 0.737 - ETA: 3:27 - loss: 0.9404 - acc: 0.737 - ETA: 3:16 - loss: 0.9374 - acc: 0.739 - ETA: 3:04 - loss: 0.9382 - acc: 0.739 - ETA: 2:53 - loss: 0.9365 - acc: 0.740 - ETA: 2:42 - loss: 0.9354 - acc: 0.740 - ETA: 2:31 - loss: 0.9344 - acc: 0.742 - ETA: 2:20 - loss: 0.9325 - acc: 0.743 - ETA: 2:08 - loss: 0.9325 - acc: 0.742 - ETA: 1:56 - loss: 0.9317 - acc: 0.742 - ETA: 1:44 - loss: 0.9302 - acc: 0.742 - ETA: 1:33 - loss: 0.9298 - acc: 0.742 - ETA: 1:21 - loss: 0.9282 - acc: 0.743 - ETA: 1:09 - loss: 0.9292 - acc: 0.742 - ETA: 58s - loss: 0.9288 - acc: 0.742 - ETA: 46s - loss: 0.9278 - acc: 0.74 - ETA: 34s - loss: 0.9274 - acc: 0.74 - ETA: 23s - loss: 0.9273 - acc: 0.74 - ETA: 11s - loss: 0.9252 - acc: 0.74 - 1430s 15s/step - loss: 0.9241 - acc: 0.7455 - val_loss: 0.9113 - val_acc: 0.6697\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 17:17 - loss: 0.7396 - acc: 0.78 - ETA: 18:26 - loss: 0.8011 - acc: 0.75 - ETA: 17:58 - loss: 0.8329 - acc: 0.77 - ETA: 17:21 - loss: 0.8330 - acc: 0.75 - ETA: 16:47 - loss: 0.8565 - acc: 0.73 - ETA: 16:25 - loss: 0.8686 - acc: 0.72 - ETA: 16:06 - loss: 0.8595 - acc: 0.74 - ETA: 15:47 - loss: 0.8630 - acc: 0.75 - ETA: 15:32 - loss: 0.8696 - acc: 0.74 - ETA: 15:16 - loss: 0.8731 - acc: 0.74 - ETA: 15:02 - loss: 0.8658 - acc: 0.75 - ETA: 14:50 - loss: 0.8684 - acc: 0.74 - ETA: 14:35 - loss: 0.8670 - acc: 0.75 - ETA: 14:24 - loss: 0.8642 - acc: 0.75 - ETA: 14:12 - loss: 0.8525 - acc: 0.76 - ETA: 13:59 - loss: 0.8507 - acc: 0.76 - ETA: 13:47 - loss: 0.8440 - acc: 0.76 - ETA: 13:36 - loss: 0.8406 - acc: 0.76 - ETA: 13:24 - loss: 0.8355 - acc: 0.77 - ETA: 13:13 - loss: 0.8389 - acc: 0.76 - ETA: 13:02 - loss: 0.8371 - acc: 0.77 - ETA: 12:50 - loss: 0.8375 - acc: 0.76 - ETA: 12:39 - loss: 0.8356 - acc: 0.76 - ETA: 12:29 - loss: 0.8317 - acc: 0.77 - ETA: 12:17 - loss: 0.8303 - acc: 0.77 - ETA: 12:07 - loss: 0.8276 - acc: 0.77 - ETA: 11:55 - loss: 0.8288 - acc: 0.77 - ETA: 11:43 - loss: 0.8255 - acc: 0.77 - ETA: 11:31 - loss: 0.8275 - acc: 0.77 - ETA: 11:19 - loss: 0.8272 - acc: 0.76 - ETA: 11:08 - loss: 0.8214 - acc: 0.77 - ETA: 10:57 - loss: 0.8224 - acc: 0.77 - ETA: 10:46 - loss: 0.8256 - acc: 0.77 - ETA: 10:36 - loss: 0.8254 - acc: 0.77 - ETA: 10:25 - loss: 0.8200 - acc: 0.77 - ETA: 10:14 - loss: 0.8158 - acc: 0.77 - ETA: 10:03 - loss: 0.8202 - acc: 0.77 - ETA: 9:51 - loss: 0.8235 - acc: 0.7714 - ETA: 9:41 - loss: 0.8250 - acc: 0.770 - ETA: 9:30 - loss: 0.8271 - acc: 0.771 - ETA: 9:18 - loss: 0.8269 - acc: 0.772 - ETA: 9:07 - loss: 0.8286 - acc: 0.771 - ETA: 8:57 - loss: 0.8253 - acc: 0.773 - ETA: 8:46 - loss: 0.8259 - acc: 0.773 - ETA: 8:35 - loss: 0.8246 - acc: 0.772 - ETA: 8:24 - loss: 0.8238 - acc: 0.773 - ETA: 8:14 - loss: 0.8205 - acc: 0.775 - ETA: 8:03 - loss: 0.8177 - acc: 0.778 - ETA: 7:52 - loss: 0.8159 - acc: 0.779 - ETA: 7:42 - loss: 0.8155 - acc: 0.780 - ETA: 7:31 - loss: 0.8131 - acc: 0.781 - ETA: 7:21 - loss: 0.8115 - acc: 0.781 - ETA: 7:10 - loss: 0.8085 - acc: 0.782 - ETA: 6:59 - loss: 0.8109 - acc: 0.782 - ETA: 6:49 - loss: 0.8084 - acc: 0.784 - ETA: 6:38 - loss: 0.8104 - acc: 0.781 - ETA: 6:28 - loss: 0.8123 - acc: 0.782 - ETA: 6:18 - loss: 0.8139 - acc: 0.781 - ETA: 6:07 - loss: 0.8131 - acc: 0.782 - ETA: 5:57 - loss: 0.8140 - acc: 0.782 - ETA: 5:47 - loss: 0.8140 - acc: 0.781 - ETA: 5:36 - loss: 0.8152 - acc: 0.779 - ETA: 5:26 - loss: 0.8164 - acc: 0.780 - ETA: 5:16 - loss: 0.8170 - acc: 0.779 - ETA: 5:05 - loss: 0.8199 - acc: 0.776 - ETA: 4:55 - loss: 0.8214 - acc: 0.776 - ETA: 4:45 - loss: 0.8207 - acc: 0.776 - ETA: 4:34 - loss: 0.8198 - acc: 0.775 - ETA: 4:24 - loss: 0.8194 - acc: 0.774 - ETA: 4:14 - loss: 0.8195 - acc: 0.775 - ETA: 4:04 - loss: 0.8188 - acc: 0.775 - ETA: 3:53 - loss: 0.8189 - acc: 0.774 - ETA: 3:43 - loss: 0.8187 - acc: 0.774 - ETA: 3:33 - loss: 0.8171 - acc: 0.776 - ETA: 3:22 - loss: 0.8153 - acc: 0.777 - ETA: 3:12 - loss: 0.8170 - acc: 0.777 - ETA: 3:02 - loss: 0.8152 - acc: 0.777 - ETA: 2:52 - loss: 0.8148 - acc: 0.777 - ETA: 2:42 - loss: 0.8162 - acc: 0.777 - ETA: 2:31 - loss: 0.8159 - acc: 0.776 - ETA: 2:21 - loss: 0.8188 - acc: 0.775 - ETA: 2:11 - loss: 0.8194 - acc: 0.775 - ETA: 2:01 - loss: 0.8184 - acc: 0.775 - ETA: 1:50 - loss: 0.8164 - acc: 0.776 - ETA: 1:40 - loss: 0.8157 - acc: 0.775 - ETA: 1:30 - loss: 0.8154 - acc: 0.775 - ETA: 1:20 - loss: 0.8165 - acc: 0.774 - ETA: 1:10 - loss: 0.8162 - acc: 0.774 - ETA: 1:00 - loss: 0.8173 - acc: 0.773 - ETA: 50s - loss: 0.8159 - acc: 0.775 - ETA: 40s - loss: 0.8132 - acc: 0.77 - ETA: 30s - loss: 0.8134 - acc: 0.77 - ETA: 20s - loss: 0.8120 - acc: 0.77 - ETA: 10s - loss: 0.8121 - acc: 0.77 - 1264s 13s/step - loss: 0.8131 - acc: 0.7744 - val_loss: 0.8554 - val_acc: 0.6975\n",
      "Epoch 6/100\n",
      "95/95 [==============================] - ETA: 15:27 - loss: 0.5878 - acc: 0.90 - ETA: 15:22 - loss: 0.6172 - acc: 0.89 - ETA: 15:14 - loss: 0.7140 - acc: 0.82 - ETA: 14:59 - loss: 0.7359 - acc: 0.81 - ETA: 14:51 - loss: 0.7408 - acc: 0.81 - ETA: 14:40 - loss: 0.7430 - acc: 0.80 - ETA: 14:30 - loss: 0.7458 - acc: 0.80 - ETA: 14:21 - loss: 0.7414 - acc: 0.80 - ETA: 14:10 - loss: 0.7510 - acc: 0.80 - ETA: 14:01 - loss: 0.7618 - acc: 0.79 - ETA: 13:51 - loss: 0.7643 - acc: 0.79 - ETA: 13:40 - loss: 0.7709 - acc: 0.79 - ETA: 13:32 - loss: 0.7772 - acc: 0.79 - ETA: 13:21 - loss: 0.7704 - acc: 0.79 - ETA: 13:11 - loss: 0.7712 - acc: 0.78 - ETA: 13:02 - loss: 0.7757 - acc: 0.78 - ETA: 12:51 - loss: 0.7711 - acc: 0.78 - ETA: 12:41 - loss: 0.7754 - acc: 0.77 - ETA: 12:31 - loss: 0.7813 - acc: 0.77 - ETA: 12:21 - loss: 0.7839 - acc: 0.77 - ETA: 12:12 - loss: 0.7904 - acc: 0.77 - ETA: 12:01 - loss: 0.7808 - acc: 0.77 - ETA: 11:51 - loss: 0.7852 - acc: 0.77 - ETA: 11:42 - loss: 0.7893 - acc: 0.77 - ETA: 11:31 - loss: 0.7824 - acc: 0.77 - ETA: 11:22 - loss: 0.7780 - acc: 0.77 - ETA: 11:12 - loss: 0.7733 - acc: 0.78 - ETA: 11:02 - loss: 0.7767 - acc: 0.77 - ETA: 10:52 - loss: 0.7759 - acc: 0.77 - ETA: 10:42 - loss: 0.7729 - acc: 0.77 - ETA: 10:32 - loss: 0.7761 - acc: 0.77 - ETA: 10:17 - loss: 0.7742 - acc: 0.77 - ETA: 10:07 - loss: 0.7765 - acc: 0.77 - ETA: 9:58 - loss: 0.7750 - acc: 0.7688 - ETA: 9:48 - loss: 0.7754 - acc: 0.766 - ETA: 9:39 - loss: 0.7768 - acc: 0.765 - ETA: 9:29 - loss: 0.7741 - acc: 0.766 - ETA: 9:19 - loss: 0.7684 - acc: 0.771 - ETA: 9:10 - loss: 0.7669 - acc: 0.771 - ETA: 9:00 - loss: 0.7663 - acc: 0.772 - ETA: 8:50 - loss: 0.7655 - acc: 0.773 - ETA: 8:40 - loss: 0.7619 - acc: 0.774 - ETA: 8:30 - loss: 0.7600 - acc: 0.773 - ETA: 8:21 - loss: 0.7597 - acc: 0.775 - ETA: 8:11 - loss: 0.7602 - acc: 0.775 - ETA: 8:01 - loss: 0.7592 - acc: 0.776 - ETA: 7:52 - loss: 0.7588 - acc: 0.776 - ETA: 7:42 - loss: 0.7591 - acc: 0.776 - ETA: 7:32 - loss: 0.7566 - acc: 0.778 - ETA: 7:22 - loss: 0.7584 - acc: 0.777 - ETA: 7:12 - loss: 0.7562 - acc: 0.779 - ETA: 7:03 - loss: 0.7560 - acc: 0.778 - ETA: 6:53 - loss: 0.7545 - acc: 0.779 - ETA: 6:43 - loss: 0.7549 - acc: 0.778 - ETA: 6:33 - loss: 0.7552 - acc: 0.777 - ETA: 6:23 - loss: 0.7536 - acc: 0.779 - ETA: 6:14 - loss: 0.7539 - acc: 0.779 - ETA: 6:04 - loss: 0.7525 - acc: 0.782 - ETA: 5:54 - loss: 0.7515 - acc: 0.785 - ETA: 5:44 - loss: 0.7510 - acc: 0.786 - ETA: 5:34 - loss: 0.7495 - acc: 0.787 - ETA: 5:24 - loss: 0.7489 - acc: 0.787 - ETA: 5:15 - loss: 0.7471 - acc: 0.788 - ETA: 5:05 - loss: 0.7499 - acc: 0.786 - ETA: 4:55 - loss: 0.7487 - acc: 0.786 - ETA: 4:45 - loss: 0.7498 - acc: 0.785 - ETA: 4:35 - loss: 0.7505 - acc: 0.785 - ETA: 4:25 - loss: 0.7508 - acc: 0.784 - ETA: 4:15 - loss: 0.7514 - acc: 0.783 - ETA: 4:06 - loss: 0.7488 - acc: 0.785 - ETA: 3:56 - loss: 0.7477 - acc: 0.785 - ETA: 3:46 - loss: 0.7498 - acc: 0.784 - ETA: 3:36 - loss: 0.7487 - acc: 0.784 - ETA: 3:26 - loss: 0.7450 - acc: 0.786 - ETA: 3:17 - loss: 0.7443 - acc: 0.786 - ETA: 3:07 - loss: 0.7441 - acc: 0.787 - ETA: 2:57 - loss: 0.7452 - acc: 0.787 - ETA: 2:47 - loss: 0.7445 - acc: 0.787 - ETA: 2:37 - loss: 0.7419 - acc: 0.788 - ETA: 2:27 - loss: 0.7433 - acc: 0.787 - ETA: 2:17 - loss: 0.7421 - acc: 0.788 - ETA: 2:08 - loss: 0.7424 - acc: 0.787 - ETA: 1:58 - loss: 0.7418 - acc: 0.788 - ETA: 1:48 - loss: 0.7407 - acc: 0.788 - ETA: 1:38 - loss: 0.7384 - acc: 0.789 - ETA: 1:28 - loss: 0.7365 - acc: 0.790 - ETA: 1:18 - loss: 0.7342 - acc: 0.792 - ETA: 1:09 - loss: 0.7352 - acc: 0.791 - ETA: 59s - loss: 0.7336 - acc: 0.792 - ETA: 49s - loss: 0.7335 - acc: 0.79 - ETA: 39s - loss: 0.7315 - acc: 0.79 - ETA: 29s - loss: 0.7333 - acc: 0.79 - ETA: 19s - loss: 0.7324 - acc: 0.79 - ETA: 9s - loss: 0.7326 - acc: 0.7927 - 1243s 13s/step - loss: 0.7333 - acc: 0.7922 - val_loss: 0.8216 - val_acc: 0.7055\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 15:51 - loss: 0.6499 - acc: 0.71 - ETA: 15:40 - loss: 0.7009 - acc: 0.71 - ETA: 15:16 - loss: 0.6570 - acc: 0.77 - ETA: 15:08 - loss: 0.6627 - acc: 0.78 - ETA: 14:59 - loss: 0.6725 - acc: 0.78 - ETA: 14:45 - loss: 0.6645 - acc: 0.78 - ETA: 14:36 - loss: 0.6501 - acc: 0.80 - ETA: 14:24 - loss: 0.6519 - acc: 0.80 - ETA: 14:15 - loss: 0.6688 - acc: 0.79 - ETA: 14:05 - loss: 0.6735 - acc: 0.78 - ETA: 13:53 - loss: 0.6748 - acc: 0.78 - ETA: 13:44 - loss: 0.6717 - acc: 0.79 - ETA: 13:33 - loss: 0.6696 - acc: 0.80 - ETA: 13:23 - loss: 0.6799 - acc: 0.80 - ETA: 13:13 - loss: 0.6701 - acc: 0.80 - ETA: 13:02 - loss: 0.6678 - acc: 0.80 - ETA: 12:53 - loss: 0.6664 - acc: 0.80 - ETA: 12:44 - loss: 0.6719 - acc: 0.80 - ETA: 12:33 - loss: 0.6739 - acc: 0.80 - ETA: 12:24 - loss: 0.6747 - acc: 0.80 - ETA: 12:13 - loss: 0.6795 - acc: 0.79 - ETA: 12:03 - loss: 0.6804 - acc: 0.79 - ETA: 11:54 - loss: 0.6821 - acc: 0.79 - ETA: 11:43 - loss: 0.6862 - acc: 0.79 - ETA: 11:34 - loss: 0.6888 - acc: 0.78 - ETA: 11:24 - loss: 0.6940 - acc: 0.78 - ETA: 11:13 - loss: 0.6907 - acc: 0.78 - ETA: 11:04 - loss: 0.6870 - acc: 0.79 - ETA: 10:53 - loss: 0.6818 - acc: 0.79 - ETA: 10:43 - loss: 0.6814 - acc: 0.79 - ETA: 10:34 - loss: 0.6794 - acc: 0.79 - ETA: 10:23 - loss: 0.6786 - acc: 0.79 - ETA: 10:14 - loss: 0.6742 - acc: 0.79 - ETA: 10:04 - loss: 0.6734 - acc: 0.79 - ETA: 9:54 - loss: 0.6731 - acc: 0.7973 - ETA: 9:44 - loss: 0.6672 - acc: 0.800 - ETA: 9:34 - loss: 0.6713 - acc: 0.797 - ETA: 9:24 - loss: 0.6738 - acc: 0.796 - ETA: 9:14 - loss: 0.6761 - acc: 0.794 - ETA: 9:04 - loss: 0.6760 - acc: 0.796 - ETA: 8:54 - loss: 0.6771 - acc: 0.795 - ETA: 8:44 - loss: 0.6777 - acc: 0.794 - ETA: 8:34 - loss: 0.6778 - acc: 0.797 - ETA: 8:24 - loss: 0.6755 - acc: 0.798 - ETA: 8:14 - loss: 0.6733 - acc: 0.802 - ETA: 8:04 - loss: 0.6728 - acc: 0.802 - ETA: 7:54 - loss: 0.6729 - acc: 0.803 - ETA: 7:45 - loss: 0.6725 - acc: 0.804 - ETA: 7:35 - loss: 0.6716 - acc: 0.803 - ETA: 7:25 - loss: 0.6689 - acc: 0.805 - ETA: 7:15 - loss: 0.6667 - acc: 0.807 - ETA: 7:05 - loss: 0.6682 - acc: 0.807 - ETA: 6:55 - loss: 0.6676 - acc: 0.807 - ETA: 6:45 - loss: 0.6653 - acc: 0.810 - ETA: 6:35 - loss: 0.6656 - acc: 0.809 - ETA: 6:26 - loss: 0.6726 - acc: 0.806 - ETA: 6:16 - loss: 0.6704 - acc: 0.808 - ETA: 6:06 - loss: 0.6675 - acc: 0.809 - ETA: 5:56 - loss: 0.6677 - acc: 0.808 - ETA: 5:47 - loss: 0.6657 - acc: 0.808 - ETA: 5:37 - loss: 0.6665 - acc: 0.806 - ETA: 5:28 - loss: 0.6642 - acc: 0.807 - ETA: 5:18 - loss: 0.6651 - acc: 0.807 - ETA: 5:08 - loss: 0.6644 - acc: 0.807 - ETA: 4:58 - loss: 0.6634 - acc: 0.807 - ETA: 4:48 - loss: 0.6650 - acc: 0.807 - ETA: 4:38 - loss: 0.6617 - acc: 0.809 - ETA: 4:28 - loss: 0.6615 - acc: 0.808 - ETA: 4:18 - loss: 0.6612 - acc: 0.808 - ETA: 4:08 - loss: 0.6615 - acc: 0.809 - ETA: 3:58 - loss: 0.6612 - acc: 0.810 - ETA: 3:48 - loss: 0.6600 - acc: 0.810 - ETA: 3:38 - loss: 0.6602 - acc: 0.809 - ETA: 3:28 - loss: 0.6588 - acc: 0.810 - ETA: 3:18 - loss: 0.6575 - acc: 0.810 - ETA: 3:08 - loss: 0.6592 - acc: 0.810 - ETA: 2:58 - loss: 0.6588 - acc: 0.810 - ETA: 2:48 - loss: 0.6582 - acc: 0.811 - ETA: 2:39 - loss: 0.6589 - acc: 0.810 - ETA: 2:29 - loss: 0.6592 - acc: 0.810 - ETA: 2:19 - loss: 0.6593 - acc: 0.810 - ETA: 2:09 - loss: 0.6593 - acc: 0.810 - ETA: 1:59 - loss: 0.6601 - acc: 0.809 - ETA: 1:49 - loss: 0.6622 - acc: 0.808 - ETA: 1:39 - loss: 0.6645 - acc: 0.807 - ETA: 1:29 - loss: 0.6629 - acc: 0.808 - ETA: 1:19 - loss: 0.6620 - acc: 0.808 - ETA: 1:09 - loss: 0.6618 - acc: 0.809 - ETA: 59s - loss: 0.6630 - acc: 0.808 - ETA: 49s - loss: 0.6646 - acc: 0.80 - ETA: 39s - loss: 0.6641 - acc: 0.80 - ETA: 29s - loss: 0.6653 - acc: 0.80 - ETA: 19s - loss: 0.6645 - acc: 0.80 - ETA: 9s - loss: 0.6630 - acc: 0.8083 - 1281s 13s/step - loss: 0.6626 - acc: 0.8080 - val_loss: 0.7939 - val_acc: 0.7159\n",
      "Epoch 8/100\n",
      "95/95 [==============================] - ETA: 18:44 - loss: 0.7930 - acc: 0.75 - ETA: 18:12 - loss: 0.7278 - acc: 0.79 - ETA: 18:08 - loss: 0.7084 - acc: 0.82 - ETA: 17:18 - loss: 0.7166 - acc: 0.82 - ETA: 16:00 - loss: 0.7062 - acc: 0.83 - ETA: 15:51 - loss: 0.7062 - acc: 0.81 - ETA: 15:34 - loss: 0.7284 - acc: 0.80 - ETA: 15:20 - loss: 0.7168 - acc: 0.80 - ETA: 15:07 - loss: 0.6969 - acc: 0.82 - ETA: 15:05 - loss: 0.6724 - acc: 0.83 - ETA: 15:00 - loss: 0.6717 - acc: 0.83 - ETA: 14:49 - loss: 0.6685 - acc: 0.82 - ETA: 14:39 - loss: 0.6640 - acc: 0.82 - ETA: 14:34 - loss: 0.6600 - acc: 0.83 - ETA: 14:24 - loss: 0.6616 - acc: 0.82 - ETA: 14:11 - loss: 0.6554 - acc: 0.83 - ETA: 13:57 - loss: 0.6498 - acc: 0.83 - ETA: 13:45 - loss: 0.6536 - acc: 0.82 - ETA: 13:30 - loss: 0.6437 - acc: 0.83 - ETA: 13:16 - loss: 0.6357 - acc: 0.83 - ETA: 13:03 - loss: 0.6344 - acc: 0.83 - ETA: 12:54 - loss: 0.6313 - acc: 0.83 - ETA: 12:46 - loss: 0.6293 - acc: 0.83 - ETA: 12:35 - loss: 0.6313 - acc: 0.83 - ETA: 12:23 - loss: 0.6314 - acc: 0.83 - ETA: 12:11 - loss: 0.6283 - acc: 0.83 - ETA: 11:59 - loss: 0.6270 - acc: 0.83 - ETA: 11:49 - loss: 0.6326 - acc: 0.83 - ETA: 11:39 - loss: 0.6347 - acc: 0.83 - ETA: 11:28 - loss: 0.6414 - acc: 0.82 - ETA: 11:18 - loss: 0.6398 - acc: 0.82 - ETA: 11:06 - loss: 0.6400 - acc: 0.82 - ETA: 10:55 - loss: 0.6407 - acc: 0.82 - ETA: 10:44 - loss: 0.6412 - acc: 0.82 - ETA: 10:35 - loss: 0.6378 - acc: 0.82 - ETA: 10:26 - loss: 0.6362 - acc: 0.82 - ETA: 10:17 - loss: 0.6360 - acc: 0.82 - ETA: 10:07 - loss: 0.6335 - acc: 0.82 - ETA: 9:57 - loss: 0.6419 - acc: 0.8212 - ETA: 9:47 - loss: 0.6461 - acc: 0.818 - ETA: 9:36 - loss: 0.6455 - acc: 0.817 - ETA: 9:25 - loss: 0.6423 - acc: 0.819 - ETA: 9:13 - loss: 0.6421 - acc: 0.819 - ETA: 9:02 - loss: 0.6422 - acc: 0.818 - ETA: 8:53 - loss: 0.6446 - acc: 0.816 - ETA: 8:43 - loss: 0.6452 - acc: 0.815 - ETA: 8:34 - loss: 0.6427 - acc: 0.815 - ETA: 8:25 - loss: 0.6423 - acc: 0.815 - ETA: 8:14 - loss: 0.6422 - acc: 0.816 - ETA: 8:04 - loss: 0.6404 - acc: 0.816 - ETA: 7:56 - loss: 0.6397 - acc: 0.816 - ETA: 7:47 - loss: 0.6399 - acc: 0.815 - ETA: 7:37 - loss: 0.6389 - acc: 0.815 - ETA: 7:27 - loss: 0.6387 - acc: 0.814 - ETA: 7:17 - loss: 0.6382 - acc: 0.814 - ETA: 7:06 - loss: 0.6375 - acc: 0.815 - ETA: 6:56 - loss: 0.6365 - acc: 0.815 - ETA: 6:45 - loss: 0.6425 - acc: 0.811 - ETA: 6:35 - loss: 0.6470 - acc: 0.808 - ETA: 6:24 - loss: 0.6421 - acc: 0.810 - ETA: 6:13 - loss: 0.6392 - acc: 0.812 - ETA: 6:03 - loss: 0.6373 - acc: 0.812 - ETA: 5:52 - loss: 0.6369 - acc: 0.813 - ETA: 5:41 - loss: 0.6353 - acc: 0.813 - ETA: 5:30 - loss: 0.6372 - acc: 0.812 - ETA: 5:20 - loss: 0.6386 - acc: 0.812 - ETA: 5:09 - loss: 0.6384 - acc: 0.813 - ETA: 4:58 - loss: 0.6372 - acc: 0.813 - ETA: 4:47 - loss: 0.6387 - acc: 0.812 - ETA: 4:36 - loss: 0.6377 - acc: 0.812 - ETA: 4:25 - loss: 0.6382 - acc: 0.812 - ETA: 4:14 - loss: 0.6392 - acc: 0.811 - ETA: 4:03 - loss: 0.6390 - acc: 0.811 - ETA: 3:52 - loss: 0.6381 - acc: 0.812 - ETA: 3:41 - loss: 0.6404 - acc: 0.809 - ETA: 3:30 - loss: 0.6371 - acc: 0.811 - ETA: 3:19 - loss: 0.6373 - acc: 0.811 - ETA: 3:08 - loss: 0.6367 - acc: 0.812 - ETA: 2:57 - loss: 0.6384 - acc: 0.812 - ETA: 2:46 - loss: 0.6382 - acc: 0.812 - ETA: 2:35 - loss: 0.6377 - acc: 0.812 - ETA: 2:24 - loss: 0.6399 - acc: 0.811 - ETA: 2:13 - loss: 0.6386 - acc: 0.812 - ETA: 2:02 - loss: 0.6373 - acc: 0.812 - ETA: 1:51 - loss: 0.6386 - acc: 0.812 - ETA: 1:40 - loss: 0.6386 - acc: 0.812 - ETA: 1:29 - loss: 0.6385 - acc: 0.812 - ETA: 1:17 - loss: 0.6388 - acc: 0.812 - ETA: 1:06 - loss: 0.6373 - acc: 0.812 - ETA: 55s - loss: 0.6376 - acc: 0.812 - ETA: 44s - loss: 0.6385 - acc: 0.81 - ETA: 33s - loss: 0.6366 - acc: 0.81 - ETA: 22s - loss: 0.6363 - acc: 0.81 - ETA: 11s - loss: 0.6363 - acc: 0.81 - 1413s 15s/step - loss: 0.6355 - acc: 0.8134 - val_loss: 0.7611 - val_acc: 0.7263\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 18:31 - loss: 0.5730 - acc: 0.81 - ETA: 17:47 - loss: 0.5430 - acc: 0.82 - ETA: 17:34 - loss: 0.5413 - acc: 0.83 - ETA: 17:24 - loss: 0.5586 - acc: 0.82 - ETA: 17:09 - loss: 0.5576 - acc: 0.83 - ETA: 17:23 - loss: 0.5755 - acc: 0.81 - ETA: 17:12 - loss: 0.5944 - acc: 0.80 - ETA: 17:03 - loss: 0.6290 - acc: 0.78 - ETA: 17:03 - loss: 0.6157 - acc: 0.79 - ETA: 16:46 - loss: 0.6014 - acc: 0.80 - ETA: 16:37 - loss: 0.6127 - acc: 0.80 - ETA: 16:16 - loss: 0.5959 - acc: 0.80 - ETA: 15:53 - loss: 0.6009 - acc: 0.81 - ETA: 15:35 - loss: 0.5955 - acc: 0.81 - ETA: 15:17 - loss: 0.5951 - acc: 0.81 - ETA: 14:59 - loss: 0.5950 - acc: 0.81 - ETA: 14:43 - loss: 0.6012 - acc: 0.81 - ETA: 14:27 - loss: 0.5972 - acc: 0.81 - ETA: 14:11 - loss: 0.5974 - acc: 0.81 - ETA: 13:57 - loss: 0.5978 - acc: 0.81 - ETA: 13:43 - loss: 0.5971 - acc: 0.81 - ETA: 13:30 - loss: 0.6088 - acc: 0.80 - ETA: 13:17 - loss: 0.6094 - acc: 0.80 - ETA: 13:03 - loss: 0.6108 - acc: 0.80 - ETA: 12:50 - loss: 0.6028 - acc: 0.81 - ETA: 12:37 - loss: 0.5998 - acc: 0.81 - ETA: 12:24 - loss: 0.5985 - acc: 0.81 - ETA: 12:12 - loss: 0.5973 - acc: 0.81 - ETA: 12:00 - loss: 0.5923 - acc: 0.81 - ETA: 11:48 - loss: 0.5888 - acc: 0.81 - ETA: 11:42 - loss: 0.5847 - acc: 0.82 - ETA: 11:30 - loss: 0.5811 - acc: 0.82 - ETA: 11:19 - loss: 0.5863 - acc: 0.82 - ETA: 11:07 - loss: 0.5858 - acc: 0.81 - ETA: 10:56 - loss: 0.5800 - acc: 0.82 - ETA: 10:44 - loss: 0.5821 - acc: 0.82 - ETA: 10:31 - loss: 0.5826 - acc: 0.82 - ETA: 10:19 - loss: 0.5804 - acc: 0.82 - ETA: 10:07 - loss: 0.5794 - acc: 0.82 - ETA: 9:55 - loss: 0.5810 - acc: 0.8227 - ETA: 9:44 - loss: 0.5866 - acc: 0.820 - ETA: 9:32 - loss: 0.5868 - acc: 0.820 - ETA: 9:17 - loss: 0.5850 - acc: 0.820 - ETA: 9:05 - loss: 0.5863 - acc: 0.819 - ETA: 8:54 - loss: 0.5842 - acc: 0.821 - ETA: 8:42 - loss: 0.5829 - acc: 0.823 - ETA: 8:33 - loss: 0.5831 - acc: 0.822 - ETA: 8:23 - loss: 0.5805 - acc: 0.824 - ETA: 8:13 - loss: 0.5763 - acc: 0.826 - ETA: 8:03 - loss: 0.5817 - acc: 0.824 - ETA: 7:53 - loss: 0.5802 - acc: 0.824 - ETA: 7:42 - loss: 0.5792 - acc: 0.824 - ETA: 7:32 - loss: 0.5752 - acc: 0.826 - ETA: 7:22 - loss: 0.5756 - acc: 0.827 - ETA: 7:12 - loss: 0.5757 - acc: 0.827 - ETA: 7:01 - loss: 0.5729 - acc: 0.828 - ETA: 6:50 - loss: 0.5734 - acc: 0.828 - ETA: 6:40 - loss: 0.5731 - acc: 0.828 - ETA: 6:29 - loss: 0.5728 - acc: 0.828 - ETA: 6:19 - loss: 0.5726 - acc: 0.828 - ETA: 6:08 - loss: 0.5744 - acc: 0.826 - ETA: 5:57 - loss: 0.5741 - acc: 0.827 - ETA: 5:47 - loss: 0.5718 - acc: 0.829 - ETA: 5:37 - loss: 0.5727 - acc: 0.829 - ETA: 5:26 - loss: 0.5780 - acc: 0.827 - ETA: 5:15 - loss: 0.5784 - acc: 0.827 - ETA: 5:04 - loss: 0.5782 - acc: 0.828 - ETA: 4:54 - loss: 0.5773 - acc: 0.829 - ETA: 4:43 - loss: 0.5785 - acc: 0.828 - ETA: 4:32 - loss: 0.5764 - acc: 0.829 - ETA: 4:21 - loss: 0.5765 - acc: 0.829 - ETA: 4:10 - loss: 0.5780 - acc: 0.827 - ETA: 4:00 - loss: 0.5788 - acc: 0.826 - ETA: 3:49 - loss: 0.5796 - acc: 0.825 - ETA: 3:38 - loss: 0.5810 - acc: 0.823 - ETA: 3:27 - loss: 0.5803 - acc: 0.824 - ETA: 3:16 - loss: 0.5808 - acc: 0.823 - ETA: 3:05 - loss: 0.5821 - acc: 0.823 - ETA: 2:54 - loss: 0.5818 - acc: 0.823 - ETA: 2:43 - loss: 0.5811 - acc: 0.823 - ETA: 2:33 - loss: 0.5798 - acc: 0.824 - ETA: 2:22 - loss: 0.5785 - acc: 0.825 - ETA: 2:11 - loss: 0.5777 - acc: 0.825 - ETA: 2:00 - loss: 0.5781 - acc: 0.824 - ETA: 1:49 - loss: 0.5784 - acc: 0.824 - ETA: 1:38 - loss: 0.5808 - acc: 0.824 - ETA: 1:27 - loss: 0.5809 - acc: 0.824 - ETA: 1:16 - loss: 0.5822 - acc: 0.824 - ETA: 1:05 - loss: 0.5832 - acc: 0.824 - ETA: 54s - loss: 0.5825 - acc: 0.824 - ETA: 43s - loss: 0.5801 - acc: 0.82 - ETA: 32s - loss: 0.5788 - acc: 0.82 - ETA: 21s - loss: 0.5793 - acc: 0.82 - ETA: 10s - loss: 0.5792 - acc: 0.82 - 1347s 14s/step - loss: 0.5796 - acc: 0.8268 - val_loss: 0.7355 - val_acc: 0.7321\n",
      "Epoch 10/100\n",
      "95/95 [==============================] - ETA: 16:30 - loss: 0.4677 - acc: 0.87 - ETA: 15:56 - loss: 0.5368 - acc: 0.85 - ETA: 15:54 - loss: 0.5119 - acc: 0.86 - ETA: 15:42 - loss: 0.4725 - acc: 0.87 - ETA: 15:24 - loss: 0.5024 - acc: 0.86 - ETA: 15:14 - loss: 0.5211 - acc: 0.85 - ETA: 15:04 - loss: 0.5170 - acc: 0.85 - ETA: 14:51 - loss: 0.5200 - acc: 0.85 - ETA: 14:40 - loss: 0.5264 - acc: 0.85 - ETA: 14:29 - loss: 0.5282 - acc: 0.84 - ETA: 14:18 - loss: 0.5312 - acc: 0.84 - ETA: 14:08 - loss: 0.5339 - acc: 0.85 - ETA: 13:56 - loss: 0.5246 - acc: 0.86 - ETA: 13:46 - loss: 0.5222 - acc: 0.86 - ETA: 13:43 - loss: 0.5277 - acc: 0.85 - ETA: 13:18 - loss: 0.5326 - acc: 0.85 - ETA: 13:08 - loss: 0.5313 - acc: 0.85 - ETA: 12:58 - loss: 0.5303 - acc: 0.85 - ETA: 12:48 - loss: 0.5257 - acc: 0.85 - ETA: 12:38 - loss: 0.5155 - acc: 0.86 - ETA: 12:28 - loss: 0.5206 - acc: 0.85 - ETA: 12:18 - loss: 0.5218 - acc: 0.85 - ETA: 12:08 - loss: 0.5180 - acc: 0.85 - ETA: 11:57 - loss: 0.5187 - acc: 0.85 - ETA: 11:47 - loss: 0.5192 - acc: 0.85 - ETA: 11:38 - loss: 0.5164 - acc: 0.85 - ETA: 11:27 - loss: 0.5184 - acc: 0.85 - ETA: 11:18 - loss: 0.5143 - acc: 0.85 - ETA: 11:07 - loss: 0.5216 - acc: 0.85 - ETA: 10:57 - loss: 0.5225 - acc: 0.85 - ETA: 10:47 - loss: 0.5277 - acc: 0.84 - ETA: 10:37 - loss: 0.5283 - acc: 0.85 - ETA: 10:27 - loss: 0.5241 - acc: 0.85 - ETA: 10:17 - loss: 0.5218 - acc: 0.85 - ETA: 10:06 - loss: 0.5256 - acc: 0.85 - ETA: 9:56 - loss: 0.5294 - acc: 0.8511 - ETA: 9:46 - loss: 0.5370 - acc: 0.847 - ETA: 9:36 - loss: 0.5344 - acc: 0.847 - ETA: 9:26 - loss: 0.5349 - acc: 0.847 - ETA: 9:16 - loss: 0.5359 - acc: 0.848 - ETA: 9:06 - loss: 0.5384 - acc: 0.847 - ETA: 8:56 - loss: 0.5402 - acc: 0.846 - ETA: 8:46 - loss: 0.5428 - acc: 0.845 - ETA: 8:36 - loss: 0.5440 - acc: 0.844 - ETA: 8:26 - loss: 0.5436 - acc: 0.844 - ETA: 8:15 - loss: 0.5469 - acc: 0.842 - ETA: 8:05 - loss: 0.5463 - acc: 0.844 - ETA: 7:55 - loss: 0.5486 - acc: 0.841 - ETA: 7:45 - loss: 0.5478 - acc: 0.841 - ETA: 7:35 - loss: 0.5503 - acc: 0.840 - ETA: 7:25 - loss: 0.5500 - acc: 0.841 - ETA: 7:15 - loss: 0.5485 - acc: 0.842 - ETA: 7:05 - loss: 0.5526 - acc: 0.841 - ETA: 6:54 - loss: 0.5541 - acc: 0.840 - ETA: 6:44 - loss: 0.5513 - acc: 0.841 - ETA: 6:34 - loss: 0.5558 - acc: 0.838 - ETA: 6:24 - loss: 0.5560 - acc: 0.838 - ETA: 6:14 - loss: 0.5526 - acc: 0.839 - ETA: 6:04 - loss: 0.5514 - acc: 0.840 - ETA: 5:54 - loss: 0.5523 - acc: 0.839 - ETA: 5:44 - loss: 0.5520 - acc: 0.839 - ETA: 5:33 - loss: 0.5545 - acc: 0.837 - ETA: 5:23 - loss: 0.5517 - acc: 0.838 - ETA: 5:13 - loss: 0.5508 - acc: 0.838 - ETA: 5:03 - loss: 0.5505 - acc: 0.838 - ETA: 4:53 - loss: 0.5524 - acc: 0.836 - ETA: 4:43 - loss: 0.5504 - acc: 0.837 - ETA: 4:33 - loss: 0.5489 - acc: 0.838 - ETA: 4:23 - loss: 0.5477 - acc: 0.838 - ETA: 4:13 - loss: 0.5519 - acc: 0.835 - ETA: 4:03 - loss: 0.5503 - acc: 0.836 - ETA: 3:53 - loss: 0.5497 - acc: 0.837 - ETA: 3:43 - loss: 0.5528 - acc: 0.835 - ETA: 3:33 - loss: 0.5521 - acc: 0.835 - ETA: 3:22 - loss: 0.5514 - acc: 0.834 - ETA: 3:12 - loss: 0.5496 - acc: 0.834 - ETA: 3:02 - loss: 0.5491 - acc: 0.834 - ETA: 2:52 - loss: 0.5491 - acc: 0.834 - ETA: 2:42 - loss: 0.5477 - acc: 0.835 - ETA: 2:32 - loss: 0.5473 - acc: 0.836 - ETA: 2:22 - loss: 0.5469 - acc: 0.836 - ETA: 2:12 - loss: 0.5465 - acc: 0.836 - ETA: 2:02 - loss: 0.5463 - acc: 0.836 - ETA: 1:51 - loss: 0.5458 - acc: 0.837 - ETA: 1:41 - loss: 0.5448 - acc: 0.837 - ETA: 1:31 - loss: 0.5450 - acc: 0.837 - ETA: 1:21 - loss: 0.5455 - acc: 0.837 - ETA: 1:11 - loss: 0.5464 - acc: 0.838 - ETA: 1:01 - loss: 0.5455 - acc: 0.838 - ETA: 50s - loss: 0.5460 - acc: 0.838 - ETA: 40s - loss: 0.5475 - acc: 0.83 - ETA: 30s - loss: 0.5458 - acc: 0.83 - ETA: 20s - loss: 0.5438 - acc: 0.83 - ETA: 10s - loss: 0.5444 - acc: 0.83 - 1281s 13s/step - loss: 0.5435 - acc: 0.8393 - val_loss: 0.7154 - val_acc: 0.7367\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/95 [============================>.] - ETA: 16:14 - loss: 0.4273 - acc: 0.90 - ETA: 15:54 - loss: 0.4338 - acc: 0.89 - ETA: 15:35 - loss: 0.4990 - acc: 0.84 - ETA: 15:34 - loss: 0.4976 - acc: 0.85 - ETA: 15:21 - loss: 0.4919 - acc: 0.85 - ETA: 15:11 - loss: 0.5013 - acc: 0.84 - ETA: 15:03 - loss: 0.4848 - acc: 0.86 - ETA: 14:50 - loss: 0.4763 - acc: 0.86 - ETA: 14:41 - loss: 0.4682 - acc: 0.86 - ETA: 14:37 - loss: 0.4652 - acc: 0.86 - ETA: 14:28 - loss: 0.4879 - acc: 0.85 - ETA: 14:17 - loss: 0.4829 - acc: 0.86 - ETA: 14:07 - loss: 0.4851 - acc: 0.86 - ETA: 13:56 - loss: 0.4893 - acc: 0.86 - ETA: 13:49 - loss: 0.4941 - acc: 0.86 - ETA: 13:38 - loss: 0.4950 - acc: 0.86 - ETA: 13:27 - loss: 0.4870 - acc: 0.86 - ETA: 13:16 - loss: 0.4872 - acc: 0.85 - ETA: 13:06 - loss: 0.4867 - acc: 0.85 - ETA: 12:55 - loss: 0.4963 - acc: 0.85 - ETA: 12:45 - loss: 0.4906 - acc: 0.85 - ETA: 12:34 - loss: 0.4928 - acc: 0.85 - ETA: 12:23 - loss: 0.4926 - acc: 0.85 - ETA: 12:14 - loss: 0.5003 - acc: 0.85 - ETA: 12:03 - loss: 0.4996 - acc: 0.85 - ETA: 11:53 - loss: 0.5025 - acc: 0.85 - ETA: 11:43 - loss: 0.5088 - acc: 0.84 - ETA: 11:32 - loss: 0.5121 - acc: 0.84 - ETA: 11:22 - loss: 0.5074 - acc: 0.84 - ETA: 11:11 - loss: 0.5166 - acc: 0.84 - ETA: 11:00 - loss: 0.5188 - acc: 0.84 - ETA: 10:50 - loss: 0.5212 - acc: 0.83 - ETA: 10:40 - loss: 0.5221 - acc: 0.84 - ETA: 10:29 - loss: 0.5225 - acc: 0.83 - ETA: 10:18 - loss: 0.5249 - acc: 0.83 - ETA: 10:08 - loss: 0.5216 - acc: 0.84 - ETA: 9:57 - loss: 0.5182 - acc: 0.8429 - ETA: 9:47 - loss: 0.5224 - acc: 0.842 - ETA: 9:38 - loss: 0.5262 - acc: 0.842 - ETA: 9:29 - loss: 0.5246 - acc: 0.842 - ETA: 9:19 - loss: 0.5220 - acc: 0.843 - ETA: 9:09 - loss: 0.5274 - acc: 0.838 - ETA: 9:00 - loss: 0.5260 - acc: 0.839 - ETA: 8:51 - loss: 0.5292 - acc: 0.837 - ETA: 8:41 - loss: 0.5279 - acc: 0.839 - ETA: 8:31 - loss: 0.5275 - acc: 0.840 - ETA: 8:21 - loss: 0.5293 - acc: 0.837 - ETA: 8:12 - loss: 0.5253 - acc: 0.839 - ETA: 8:02 - loss: 0.5225 - acc: 0.840 - ETA: 7:52 - loss: 0.5220 - acc: 0.841 - ETA: 7:42 - loss: 0.5211 - acc: 0.841 - ETA: 7:31 - loss: 0.5206 - acc: 0.841 - ETA: 7:21 - loss: 0.5210 - acc: 0.840 - ETA: 7:11 - loss: 0.5197 - acc: 0.840 - ETA: 7:01 - loss: 0.5209 - acc: 0.841 - ETA: 6:51 - loss: 0.5228 - acc: 0.839 - ETA: 6:41 - loss: 0.5215 - acc: 0.841 - ETA: 6:30 - loss: 0.5201 - acc: 0.841 - ETA: 6:20 - loss: 0.5198 - acc: 0.842 - ETA: 6:09 - loss: 0.5212 - acc: 0.841 - ETA: 5:59 - loss: 0.5208 - acc: 0.841 - ETA: 5:49 - loss: 0.5207 - acc: 0.840 - ETA: 5:38 - loss: 0.5227 - acc: 0.839 - ETA: 5:28 - loss: 0.5216 - acc: 0.838 - ETA: 5:18 - loss: 0.5202 - acc: 0.839 - ETA: 5:07 - loss: 0.5197 - acc: 0.840 - ETA: 4:57 - loss: 0.5187 - acc: 0.839 - ETA: 4:46 - loss: 0.5186 - acc: 0.839 - ETA: 4:36 - loss: 0.5180 - acc: 0.838 - ETA: 4:26 - loss: 0.5166 - acc: 0.838 - ETA: 4:15 - loss: 0.5144 - acc: 0.839 - ETA: 4:04 - loss: 0.5153 - acc: 0.839 - ETA: 3:54 - loss: 0.5144 - acc: 0.840 - ETA: 3:43 - loss: 0.5157 - acc: 0.839 - ETA: 3:33 - loss: 0.5134 - acc: 0.841 - ETA: 3:22 - loss: 0.5172 - acc: 0.839 - ETA: 3:12 - loss: 0.5167 - acc: 0.839 - ETA: 3:01 - loss: 0.5169 - acc: 0.839 - ETA: 2:51 - loss: 0.5175 - acc: 0.838 - ETA: 2:40 - loss: 0.5168 - acc: 0.839 - ETA: 2:29 - loss: 0.5163 - acc: 0.839 - ETA: 2:19 - loss: 0.5155 - acc: 0.840 - ETA: 2:08 - loss: 0.5154 - acc: 0.840 - ETA: 1:57 - loss: 0.5146 - acc: 0.841 - ETA: 1:47 - loss: 0.5139 - acc: 0.841 - ETA: 1:36 - loss: 0.5148 - acc: 0.840 - ETA: 1:25 - loss: 0.5127 - acc: 0.841 - ETA: 1:14 - loss: 0.5120 - acc: 0.842 - ETA: 1:04 - loss: 0.5128 - acc: 0.841 - ETA: 53s - loss: 0.5145 - acc: 0.839 - ETA: 42s - loss: 0.5157 - acc: 0.83 - ETA: 32s - loss: 0.5143 - acc: 0.83 - ETA: 21s - loss: 0.5134 - acc: 0.83 - ETA: 10s - loss: 0.5129 - acc: 0.8392"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5bdb82f15646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    228\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    231\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1467\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=200,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0cab5c707e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot training & validation accuracy values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'output/models/weights-improvement-10-0.74.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filename)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_file, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "tensorboard.set_model(model) \n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 47 - ETA: 35 - ETA: 23 - ETA: 11 - 157s 11s/step\n"
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "preds = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08001994, 0.5968215 , 0.15215993, 0.08518913, 0.08580942],\n",
       "       [0.07988948, 0.15277135, 0.3950066 , 0.13173246, 0.24060015],\n",
       "       [0.06730071, 0.08695392, 0.48542568, 0.20274799, 0.15757175],\n",
       "       ...,\n",
       "       [0.21741334, 0.29514855, 0.21954712, 0.15516518, 0.11272575],\n",
       "       [0.0820367 , 0.36382902, 0.27622166, 0.08878279, 0.18912986],\n",
       "       [0.08959825, 0.45473355, 0.19283439, 0.07145784, 0.19137593]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_values(gen):\n",
    "    vals = []\n",
    "    for i in gen:\n",
    "        vals.append(gen[i])\n",
    "    return vals\n",
    "\n",
    "def get_classes(gen):\n",
    "    rev_gen = {}\n",
    "    for i in gen:\n",
    "        rev_gen[gen[i]] = i\n",
    "    return rev_gen\n",
    "\n",
    "def find_closest (target, sets):\n",
    "    num = -1\n",
    "    diff = 10000\n",
    "    for i in sets:\n",
    "        if abs(i-target)<diff:\n",
    "            num=i\n",
    "            diff = abs(i-target)\n",
    "    \n",
    "    return num  \n",
    "\n",
    "def get_label(cls, val):\n",
    "    return cls[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affenpinscher': 0,\n",
       " 'afghan_hound': 1,\n",
       " 'african_hunting_dog': 2,\n",
       " 'airedale': 3,\n",
       " 'american_staffordshire_terrier': 4,\n",
       " 'appenzeller': 5,\n",
       " 'australian_terrier': 6,\n",
       " 'basenji': 7,\n",
       " 'basset': 8,\n",
       " 'beagle': 9,\n",
       " 'bedlington_terrier': 10,\n",
       " 'bernese_mountain_dog': 11,\n",
       " 'black-and-tan_coonhound': 12,\n",
       " 'blenheim_spaniel': 13,\n",
       " 'bloodhound': 14,\n",
       " 'bluetick': 15,\n",
       " 'border_collie': 16,\n",
       " 'border_terrier': 17,\n",
       " 'borzoi': 18,\n",
       " 'boston_bull': 19,\n",
       " 'bouvier_des_flandres': 20,\n",
       " 'boxer': 21,\n",
       " 'brabancon_griffon': 22,\n",
       " 'briard': 23,\n",
       " 'brittany_spaniel': 24,\n",
       " 'bull_mastiff': 25,\n",
       " 'cairn': 26,\n",
       " 'cardigan': 27,\n",
       " 'chesapeake_bay_retriever': 28,\n",
       " 'chihuahua': 29,\n",
       " 'chow': 30,\n",
       " 'clumber': 31,\n",
       " 'cocker_spaniel': 32,\n",
       " 'collie': 33,\n",
       " 'curly-coated_retriever': 34,\n",
       " 'dandie_dinmont': 35,\n",
       " 'dhole': 36,\n",
       " 'dingo': 37,\n",
       " 'doberman': 38,\n",
       " 'english_foxhound': 39,\n",
       " 'english_setter': 40,\n",
       " 'english_springer': 41,\n",
       " 'entlebucher': 42,\n",
       " 'eskimo_dog': 43,\n",
       " 'flat-coated_retriever': 44,\n",
       " 'french_bulldog': 45,\n",
       " 'german_shepherd': 46,\n",
       " 'german_short-haired_pointer': 47,\n",
       " 'giant_schnauzer': 48,\n",
       " 'golden_retriever': 49,\n",
       " 'gordon_setter': 50,\n",
       " 'great_dane': 51,\n",
       " 'great_pyrenees': 52,\n",
       " 'greater_swiss_mountain_dog': 53,\n",
       " 'groenendael': 54,\n",
       " 'ibizan_hound': 55,\n",
       " 'irish_setter': 56,\n",
       " 'irish_terrier': 57,\n",
       " 'irish_water_spaniel': 58,\n",
       " 'irish_wolfhound': 59,\n",
       " 'italian_greyhound': 60,\n",
       " 'japanese_spaniel': 61,\n",
       " 'keeshond': 62,\n",
       " 'kelpie': 63,\n",
       " 'kerry_blue_terrier': 64,\n",
       " 'komondor': 65,\n",
       " 'kuvasz': 66,\n",
       " 'labrador_retriever': 67,\n",
       " 'lakeland_terrier': 68,\n",
       " 'leonberg': 69,\n",
       " 'lhasa': 70,\n",
       " 'malamute': 71,\n",
       " 'malinois': 72,\n",
       " 'maltese_dog': 73,\n",
       " 'mexican_hairless': 74,\n",
       " 'miniature_pinscher': 75,\n",
       " 'miniature_poodle': 76,\n",
       " 'miniature_schnauzer': 77,\n",
       " 'newfoundland': 78,\n",
       " 'norfolk_terrier': 79,\n",
       " 'norwegian_elkhound': 80,\n",
       " 'norwich_terrier': 81,\n",
       " 'old_english_sheepdog': 82,\n",
       " 'otterhound': 83,\n",
       " 'papillon': 84,\n",
       " 'pekinese': 85,\n",
       " 'pembroke': 86,\n",
       " 'pomeranian': 87,\n",
       " 'pug': 88,\n",
       " 'redbone': 89,\n",
       " 'rhodesian_ridgeback': 90,\n",
       " 'rottweiler': 91,\n",
       " 'saint_bernard': 92,\n",
       " 'saluki': 93,\n",
       " 'samoyed': 94,\n",
       " 'schipperke': 95,\n",
       " 'scotch_terrier': 96,\n",
       " 'scottish_deerhound': 97,\n",
       " 'sealyham_terrier': 98,\n",
       " 'shetland_sheepdog': 99,\n",
       " 'shih-tzu': 100,\n",
       " 'siberian_husky': 101,\n",
       " 'silky_terrier': 102,\n",
       " 'soft-coated_wheaten_terrier': 103,\n",
       " 'staffordshire_bullterrier': 104,\n",
       " 'standard_poodle': 105,\n",
       " 'standard_schnauzer': 106,\n",
       " 'sussex_spaniel': 107,\n",
       " 'tibetan_mastiff': 108,\n",
       " 'tibetan_terrier': 109,\n",
       " 'toy_poodle': 110,\n",
       " 'toy_terrier': 111,\n",
       " 'vizsla': 112,\n",
       " 'walker_hound': 113,\n",
       " 'weimaraner': 114,\n",
       " 'welsh_springer_spaniel': 115,\n",
       " 'west_highland_white_terrier': 116,\n",
       " 'whippet': 117,\n",
       " 'wire-haired_fox_terrier': 118,\n",
       " 'yorkshire_terrier': 119}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = get_class_values(test_generator.class_indices)\n",
    "cls = get_classes(test_generator.class_indices)\n",
    "test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c6411a701d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtesting_path_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data\\\\pre\\\\test\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_closest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_path_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "testing_path_new = \"data\\\\pre\\\\test\"\n",
    "for i in range(len(preds)):\n",
    "    label = get_label(cls, find_closest(preds[i].argmax(), vals))\n",
    "    file_path = os.path.join(testing_path_new, test_generator.filenames[i])\n",
    "    img = cv2.imread(file_path, 3)\n",
    "    plt.imshow(img)\n",
    "    plt.title(file_path+\" --- \"+ label)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
