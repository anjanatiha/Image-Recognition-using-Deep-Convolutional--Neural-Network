{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=logs\n",
    "# http://desktop-bu5qmic:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# make or reset directory\n",
    "def mk_reset_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        try:\n",
    "            shutil.rmtree(directory)\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            print(\"error:\", directory)\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            print(\"error create:\", directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting parent directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting data path data seperated by class name\n",
    "# training_dir = \"data\\\\input\\\\train\"\n",
    "# testing_dir = \"data\\\\input\\\\test\"\n",
    "# validation_dir = \"data\\\\input\\\\validation\"\n",
    "\n",
    "# # joining path with parent path \n",
    "# training_dir = os.path.join(project_dir, training_dir)\n",
    "# testing_dir = os.path.join(project_dir, testing_dir)\n",
    "# validation_dir = os.path.join(project_dir, validation_dir)\n",
    "\n",
    "\n",
    "# # setting output directory\n",
    "# model_dir =  \"data\\\\output\\\\models\"\n",
    "# log_dir = \"data\\\\output\\\\logs\"\n",
    "\n",
    "# # joining path with parent path \n",
    "# model_dir = os.path.join(project_dir, model_dir)\n",
    "# log_dir = os.path.join(project_dir, log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = r'data/training'\n",
    "validation_dir = r'data/validation'\n",
    "testing_dir = r'data/test'\n",
    "model_dir = 'output/models/'\n",
    "log_dir = \"output/logs\"\n",
    "\n",
    "model_file = model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "\n",
    "# setting training parameters\n",
    "norm = 255.0\n",
    "rescale=1./norm\n",
    "shear_range=0.2\n",
    "zoom_range=0.2\n",
    "horizontal_flip=True\n",
    "\n",
    "# setting train, test, validation parameters\n",
    "target_size=(224, 224)\n",
    "batch_size=32\n",
    "class_mode='categorical'\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "metrics=['accuracy']\n",
    "\n",
    "epochs = 50\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: output/logs\n"
     ]
    }
   ],
   "source": [
    "mk_reset_dir(model_dir)\n",
    "mk_reset_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# model_file = os.path.join(model_dir, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/models/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1097 images belonging to 10 classes.\n",
      "Found 272 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# data generator for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=rescale,\n",
    "        shear_range=shear_range,\n",
    "        zoom_range=zoom_range,\n",
    "        horizontal_flip=horizontal_flip)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        training_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "# data generator for validation\n",
    "validation_datagen = ImageDataGenerator(rescale=rescale)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "# # data generator for testing\n",
    "# test_datagen = ImageDataGenerator(rescale=rescale)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#         testing_dir,\n",
    "#         target_size=target_size,\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# importing inception model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# setting model layers specially output layer with class number\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "#loading model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all laeyrs as untrainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# chose and configure optimizer \n",
    "sgd = optimizers.Adam()\n",
    "# sgd = optimizers.SGD()\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# sgd = optimizer=SGD(lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with optimizer and loss\n",
    "model.compile(sgd, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv2d_95\n",
      "2 batch_normalization_95\n",
      "3 activation_95\n",
      "4 conv2d_96\n",
      "5 batch_normalization_96\n",
      "6 activation_96\n",
      "7 conv2d_97\n",
      "8 batch_normalization_97\n",
      "9 activation_97\n",
      "10 max_pooling2d_5\n",
      "11 conv2d_98\n",
      "12 batch_normalization_98\n",
      "13 activation_98\n",
      "14 conv2d_99\n",
      "15 batch_normalization_99\n",
      "16 activation_99\n",
      "17 max_pooling2d_6\n",
      "18 conv2d_103\n",
      "19 batch_normalization_103\n",
      "20 activation_103\n",
      "21 conv2d_101\n",
      "22 conv2d_104\n",
      "23 batch_normalization_101\n",
      "24 batch_normalization_104\n",
      "25 activation_101\n",
      "26 activation_104\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_100\n",
      "29 conv2d_102\n",
      "30 conv2d_105\n",
      "31 conv2d_106\n",
      "32 batch_normalization_100\n",
      "33 batch_normalization_102\n",
      "34 batch_normalization_105\n",
      "35 batch_normalization_106\n",
      "36 activation_100\n",
      "37 activation_102\n",
      "38 activation_105\n",
      "39 activation_106\n",
      "40 mixed0\n",
      "41 conv2d_110\n",
      "42 batch_normalization_110\n",
      "43 activation_110\n",
      "44 conv2d_108\n",
      "45 conv2d_111\n",
      "46 batch_normalization_108\n",
      "47 batch_normalization_111\n",
      "48 activation_108\n",
      "49 activation_111\n",
      "50 average_pooling2d_11\n",
      "51 conv2d_107\n",
      "52 conv2d_109\n",
      "53 conv2d_112\n",
      "54 conv2d_113\n",
      "55 batch_normalization_107\n",
      "56 batch_normalization_109\n",
      "57 batch_normalization_112\n",
      "58 batch_normalization_113\n",
      "59 activation_107\n",
      "60 activation_109\n",
      "61 activation_112\n",
      "62 activation_113\n",
      "63 mixed1\n",
      "64 conv2d_117\n",
      "65 batch_normalization_117\n",
      "66 activation_117\n",
      "67 conv2d_115\n",
      "68 conv2d_118\n",
      "69 batch_normalization_115\n",
      "70 batch_normalization_118\n",
      "71 activation_115\n",
      "72 activation_118\n",
      "73 average_pooling2d_12\n",
      "74 conv2d_114\n",
      "75 conv2d_116\n",
      "76 conv2d_119\n",
      "77 conv2d_120\n",
      "78 batch_normalization_114\n",
      "79 batch_normalization_116\n",
      "80 batch_normalization_119\n",
      "81 batch_normalization_120\n",
      "82 activation_114\n",
      "83 activation_116\n",
      "84 activation_119\n",
      "85 activation_120\n",
      "86 mixed2\n",
      "87 conv2d_122\n",
      "88 batch_normalization_122\n",
      "89 activation_122\n",
      "90 conv2d_123\n",
      "91 batch_normalization_123\n",
      "92 activation_123\n",
      "93 conv2d_121\n",
      "94 conv2d_124\n",
      "95 batch_normalization_121\n",
      "96 batch_normalization_124\n",
      "97 activation_121\n",
      "98 activation_124\n",
      "99 max_pooling2d_7\n",
      "100 mixed3\n",
      "101 conv2d_129\n",
      "102 batch_normalization_129\n",
      "103 activation_129\n",
      "104 conv2d_130\n",
      "105 batch_normalization_130\n",
      "106 activation_130\n",
      "107 conv2d_126\n",
      "108 conv2d_131\n",
      "109 batch_normalization_126\n",
      "110 batch_normalization_131\n",
      "111 activation_126\n",
      "112 activation_131\n",
      "113 conv2d_127\n",
      "114 conv2d_132\n",
      "115 batch_normalization_127\n",
      "116 batch_normalization_132\n",
      "117 activation_127\n",
      "118 activation_132\n",
      "119 average_pooling2d_13\n",
      "120 conv2d_125\n",
      "121 conv2d_128\n",
      "122 conv2d_133\n",
      "123 conv2d_134\n",
      "124 batch_normalization_125\n",
      "125 batch_normalization_128\n",
      "126 batch_normalization_133\n",
      "127 batch_normalization_134\n",
      "128 activation_125\n",
      "129 activation_128\n",
      "130 activation_133\n",
      "131 activation_134\n",
      "132 mixed4\n",
      "133 conv2d_139\n",
      "134 batch_normalization_139\n",
      "135 activation_139\n",
      "136 conv2d_140\n",
      "137 batch_normalization_140\n",
      "138 activation_140\n",
      "139 conv2d_136\n",
      "140 conv2d_141\n",
      "141 batch_normalization_136\n",
      "142 batch_normalization_141\n",
      "143 activation_136\n",
      "144 activation_141\n",
      "145 conv2d_137\n",
      "146 conv2d_142\n",
      "147 batch_normalization_137\n",
      "148 batch_normalization_142\n",
      "149 activation_137\n",
      "150 activation_142\n",
      "151 average_pooling2d_14\n",
      "152 conv2d_135\n",
      "153 conv2d_138\n",
      "154 conv2d_143\n",
      "155 conv2d_144\n",
      "156 batch_normalization_135\n",
      "157 batch_normalization_138\n",
      "158 batch_normalization_143\n",
      "159 batch_normalization_144\n",
      "160 activation_135\n",
      "161 activation_138\n",
      "162 activation_143\n",
      "163 activation_144\n",
      "164 mixed5\n",
      "165 conv2d_149\n",
      "166 batch_normalization_149\n",
      "167 activation_149\n",
      "168 conv2d_150\n",
      "169 batch_normalization_150\n",
      "170 activation_150\n",
      "171 conv2d_146\n",
      "172 conv2d_151\n",
      "173 batch_normalization_146\n",
      "174 batch_normalization_151\n",
      "175 activation_146\n",
      "176 activation_151\n",
      "177 conv2d_147\n",
      "178 conv2d_152\n",
      "179 batch_normalization_147\n",
      "180 batch_normalization_152\n",
      "181 activation_147\n",
      "182 activation_152\n",
      "183 average_pooling2d_15\n",
      "184 conv2d_145\n",
      "185 conv2d_148\n",
      "186 conv2d_153\n",
      "187 conv2d_154\n",
      "188 batch_normalization_145\n",
      "189 batch_normalization_148\n",
      "190 batch_normalization_153\n",
      "191 batch_normalization_154\n",
      "192 activation_145\n",
      "193 activation_148\n",
      "194 activation_153\n",
      "195 activation_154\n",
      "196 mixed6\n",
      "197 conv2d_159\n",
      "198 batch_normalization_159\n",
      "199 activation_159\n",
      "200 conv2d_160\n",
      "201 batch_normalization_160\n",
      "202 activation_160\n",
      "203 conv2d_156\n",
      "204 conv2d_161\n",
      "205 batch_normalization_156\n",
      "206 batch_normalization_161\n",
      "207 activation_156\n",
      "208 activation_161\n",
      "209 conv2d_157\n",
      "210 conv2d_162\n",
      "211 batch_normalization_157\n",
      "212 batch_normalization_162\n",
      "213 activation_157\n",
      "214 activation_162\n",
      "215 average_pooling2d_16\n",
      "216 conv2d_155\n",
      "217 conv2d_158\n",
      "218 conv2d_163\n",
      "219 conv2d_164\n",
      "220 batch_normalization_155\n",
      "221 batch_normalization_158\n",
      "222 batch_normalization_163\n",
      "223 batch_normalization_164\n",
      "224 activation_155\n",
      "225 activation_158\n",
      "226 activation_163\n",
      "227 activation_164\n",
      "228 mixed7\n",
      "229 conv2d_167\n",
      "230 batch_normalization_167\n",
      "231 activation_167\n",
      "232 conv2d_168\n",
      "233 batch_normalization_168\n",
      "234 activation_168\n",
      "235 conv2d_165\n",
      "236 conv2d_169\n",
      "237 batch_normalization_165\n",
      "238 batch_normalization_169\n",
      "239 activation_165\n",
      "240 activation_169\n",
      "241 conv2d_166\n",
      "242 conv2d_170\n",
      "243 batch_normalization_166\n",
      "244 batch_normalization_170\n",
      "245 activation_166\n",
      "246 activation_170\n",
      "247 max_pooling2d_8\n",
      "248 mixed8\n",
      "249 conv2d_175\n",
      "250 batch_normalization_175\n",
      "251 activation_175\n",
      "252 conv2d_172\n",
      "253 conv2d_176\n",
      "254 batch_normalization_172\n",
      "255 batch_normalization_176\n",
      "256 activation_172\n",
      "257 activation_176\n",
      "258 conv2d_173\n",
      "259 conv2d_174\n",
      "260 conv2d_177\n",
      "261 conv2d_178\n",
      "262 average_pooling2d_17\n",
      "263 conv2d_171\n",
      "264 batch_normalization_173\n",
      "265 batch_normalization_174\n",
      "266 batch_normalization_177\n",
      "267 batch_normalization_178\n",
      "268 conv2d_179\n",
      "269 batch_normalization_171\n",
      "270 activation_173\n",
      "271 activation_174\n",
      "272 activation_177\n",
      "273 activation_178\n",
      "274 batch_normalization_179\n",
      "275 activation_171\n",
      "276 mixed9_0\n",
      "277 concatenate_3\n",
      "278 activation_179\n",
      "279 mixed9\n",
      "280 conv2d_184\n",
      "281 batch_normalization_184\n",
      "282 activation_184\n",
      "283 conv2d_181\n",
      "284 conv2d_185\n",
      "285 batch_normalization_181\n",
      "286 batch_normalization_185\n",
      "287 activation_181\n",
      "288 activation_185\n",
      "289 conv2d_182\n",
      "290 conv2d_183\n",
      "291 conv2d_186\n",
      "292 conv2d_187\n",
      "293 average_pooling2d_18\n",
      "294 conv2d_180\n",
      "295 batch_normalization_182\n",
      "296 batch_normalization_183\n",
      "297 batch_normalization_186\n",
      "298 batch_normalization_187\n",
      "299 conv2d_188\n",
      "300 batch_normalization_180\n",
      "301 activation_182\n",
      "302 activation_183\n",
      "303 activation_186\n",
      "304 activation_187\n",
      "305 batch_normalization_188\n",
      "306 activation_180\n",
      "307 mixed9_1\n",
      "308 concatenate_4\n",
      "309 activation_188\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "#print layers of inception model\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the top 2 inception blocks \n",
    "# freeze first 249 layers\n",
    "# unfreeze the rest\n",
    "\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)\n",
    "\n",
    "\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "# tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, batch_size=32, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "tensorboard.set_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks_list = [checkpoint, tensorboard, early_stopping]\n",
    "callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 452s 13s/step - loss: 1.3737 - acc: 0.5936 - val_loss: 0.5100 - val_acc: 0.8419\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 457s 13s/step - loss: 0.5434 - acc: 0.8254 - val_loss: 0.4045 - val_acc: 0.8750\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 422s 12s/step - loss: 0.3881 - acc: 0.8754 - val_loss: 0.1974 - val_acc: 0.9338\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 476s 14s/step - loss: 0.3545 - acc: 0.8848 - val_loss: 0.3100 - val_acc: 0.9154\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 483s 14s/step - loss: 0.2403 - acc: 0.9192 - val_loss: 0.2803 - val_acc: 0.9118\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 473s 14s/step - loss: 0.2581 - acc: 0.9178 - val_loss: 0.2775 - val_acc: 0.9191\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 494s 14s/step - loss: 0.1853 - acc: 0.9361 - val_loss: 0.2446 - val_acc: 0.9449\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 505s 14s/step - loss: 0.2197 - acc: 0.9223 - val_loss: 0.2499 - val_acc: 0.9301\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 486s 14s/step - loss: 0.2068 - acc: 0.9344 - val_loss: 0.2689 - val_acc: 0.9375\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 490s 14s/step - loss: 0.1863 - acc: 0.9455 - val_loss: 0.1748 - val_acc: 0.9596\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 481s 14s/step - loss: 0.1991 - acc: 0.9299 - val_loss: 0.3638 - val_acc: 0.9154\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 467s 13s/step - loss: 0.2352 - acc: 0.9290 - val_loss: 0.5039 - val_acc: 0.8787\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 438s 13s/step - loss: 0.1986 - acc: 0.9393 - val_loss: 0.4169 - val_acc: 0.9191\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 416s 12s/step - loss: 0.2149 - acc: 0.9196 - val_loss: 0.3078 - val_acc: 0.9118\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 411s 12s/step - loss: 0.1947 - acc: 0.9330 - val_loss: 0.3090 - val_acc: 0.9449\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 418s 12s/step - loss: 0.1572 - acc: 0.9464 - val_loss: 0.3561 - val_acc: 0.9228\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 466s 13s/step - loss: 0.1206 - acc: 0.9567 - val_loss: 0.2365 - val_acc: 0.9485\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 481s 14s/step - loss: 0.1668 - acc: 0.9379 - val_loss: 0.2898 - val_acc: 0.9596\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 479s 14s/step - loss: 0.1291 - acc: 0.9527 - val_loss: 0.3704 - val_acc: 0.9375\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 462s 13s/step - loss: 0.1369 - acc: 0.9500 - val_loss: 0.2955 - val_acc: 0.9375\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 411s 12s/step - loss: 0.1206 - acc: 0.9558 - val_loss: 0.2292 - val_acc: 0.9596\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 418s 12s/step - loss: 0.1459 - acc: 0.9469 - val_loss: 0.6969 - val_acc: 0.8713\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 421s 12s/step - loss: 0.1617 - acc: 0.9401 - val_loss: 0.3101 - val_acc: 0.9596\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 422s 12s/step - loss: 0.1523 - acc: 0.9558 - val_loss: 0.4484 - val_acc: 0.9044\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 425s 12s/step - loss: 0.1157 - acc: 0.9634 - val_loss: 0.2409 - val_acc: 0.9706\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 424s 12s/step - loss: 0.1655 - acc: 0.9531 - val_loss: 0.2641 - val_acc: 0.9596\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 423s 12s/step - loss: 0.0851 - acc: 0.9741 - val_loss: 0.4006 - val_acc: 0.9338\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 436s 12s/step - loss: 0.1464 - acc: 0.9535 - val_loss: 0.6805 - val_acc: 0.8640\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 431s 12s/step - loss: 0.1048 - acc: 0.9656 - val_loss: 0.4388 - val_acc: 0.9154\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 408s 12s/step - loss: 0.1580 - acc: 0.9442 - val_loss: 0.2883 - val_acc: 0.9449\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 398s 11s/step - loss: 0.1656 - acc: 0.9460 - val_loss: 0.3648 - val_acc: 0.9301\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 404s 12s/step - loss: 0.1745 - acc: 0.9375 - val_loss: 0.4307 - val_acc: 0.9154\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 393s 11s/step - loss: 0.0993 - acc: 0.9714 - val_loss: 0.3910 - val_acc: 0.9301\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 394s 11s/step - loss: 0.0721 - acc: 0.9750 - val_loss: 0.1959 - val_acc: 0.9632\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 13155s 376s/step - loss: 0.1597 - acc: 0.9415 - val_loss: 0.3834 - val_acc: 0.9228\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 470s 13s/step - loss: 0.2240 - acc: 0.9308 - val_loss: 0.2402 - val_acc: 0.9596\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 441s 13s/step - loss: 0.1543 - acc: 0.9500 - val_loss: 0.2747 - val_acc: 0.9338\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 471s 13s/step - loss: 0.0948 - acc: 0.9705 - val_loss: 0.4948 - val_acc: 0.9118\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 469s 13s/step - loss: 0.0863 - acc: 0.9661 - val_loss: 0.3177 - val_acc: 0.9265\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 475s 14s/step - loss: 0.0904 - acc: 0.9746 - val_loss: 0.3094 - val_acc: 0.9485\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 459s 13s/step - loss: 0.0807 - acc: 0.9786 - val_loss: 0.3860 - val_acc: 0.9338\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 474s 14s/step - loss: 0.1361 - acc: 0.9594 - val_loss: 0.5167 - val_acc: 0.9044\n",
      "Epoch 43/50\n",
      "32/35 [==========================>...] - ETA: 29s - loss: 0.2408 - acc: 0.9266"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-47825ebd3126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train inception model\n",
    "# fine-tuning the top 2 inception blocks and top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-67f3434115ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Plot training & validation accuracy values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "95/95 [==============================] - 1897s 20s/step - loss: 0.1716 - acc: 0.9482 - val_loss: 0.9000 - val_acc: 0.8129\n",
      "Epoch 2/15\n",
      "95/95 [==============================] - 1879s 20s/step - loss: 0.1657 - acc: 0.9488 - val_loss: 0.8720 - val_acc: 0.8176\n",
      "Epoch 3/15\n",
      "95/95 [==============================] - 1830s 19s/step - loss: 0.1370 - acc: 0.9559 - val_loss: 1.0008 - val_acc: 0.7933\n"
     ]
    }
   ],
   "source": [
    "# setting retraining from old model\n",
    "model_path = \"output/models/\"+\"weights-improvement-02-0.81.hdf5\"\n",
    "epochs=15\n",
    "verbose=1\n",
    "\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# retrain by loading last good model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model\n",
    "model_path = \"output/models/\"+\"weights-improvement-01-0.81.hdf5\"\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate_generator(generator=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%s%.2f%s\"% (\"Accuracy: \", result[1]*100, \"%\"))\n",
    "print(\"%s%.2f\"% (\"Loss: \", result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best\n",
    "\n",
    "# Accuracy: 84.51%\n",
    "# Loss: 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
