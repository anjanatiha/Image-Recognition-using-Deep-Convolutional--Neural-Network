{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pred(preds, Y, val_breed, index, seq, ran):\n",
    "    leng = len(preds)\n",
    "    if seq:\n",
    "        for i in range(index):\n",
    "            if ran:\n",
    "                index = random.randint(0, leng) \n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "    else:\n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "        \n",
    "def accuracy_func(preds, Y, val_breed):\n",
    "    leng = len(preds)\n",
    "    count = 0;\n",
    "    for i in range(leng):\n",
    "        _, imagenet_class_name, prob = decode_predictions(preds, top=1)[i][0]\n",
    "        if val_breed[Y[i]] == imagenet_class_name:\n",
    "            count+=1\n",
    "    accuracy = (count/leng)*100\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "vgg_model = vgg16.VGG16(weights='imagenet')\n",
    " \n",
    "#Load the Inception_V3 model\n",
    "inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "#Load the ResNet50 model\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "#Load the MobileNet model\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'data/pre/train'\n",
    "validation_path = 'data/pre/validation'\n",
    "testing_path = 'data/pre/test'\n",
    "batch_size = 32\n",
    "target_size=(224, 224)\n",
    "norm = 255.0\n",
    "class_mode='categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2657 images belonging to 120 classes.\n",
      "Found 825 images belonging to 120 classes.\n",
      "Found 874 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./norm,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "test_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testing_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(120, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_28\n",
      "1 conv2d_941\n",
      "2 batch_normalization_941\n",
      "3 activation_1186\n",
      "4 conv2d_942\n",
      "5 batch_normalization_942\n",
      "6 activation_1187\n",
      "7 conv2d_943\n",
      "8 batch_normalization_943\n",
      "9 activation_1188\n",
      "10 max_pooling2d_46\n",
      "11 conv2d_944\n",
      "12 batch_normalization_944\n",
      "13 activation_1189\n",
      "14 conv2d_945\n",
      "15 batch_normalization_945\n",
      "16 activation_1190\n",
      "17 max_pooling2d_47\n",
      "18 conv2d_949\n",
      "19 batch_normalization_949\n",
      "20 activation_1194\n",
      "21 conv2d_947\n",
      "22 conv2d_950\n",
      "23 batch_normalization_947\n",
      "24 batch_normalization_950\n",
      "25 activation_1192\n",
      "26 activation_1195\n",
      "27 average_pooling2d_91\n",
      "28 conv2d_946\n",
      "29 conv2d_948\n",
      "30 conv2d_951\n",
      "31 conv2d_952\n",
      "32 batch_normalization_946\n",
      "33 batch_normalization_948\n",
      "34 batch_normalization_951\n",
      "35 batch_normalization_952\n",
      "36 activation_1191\n",
      "37 activation_1193\n",
      "38 activation_1196\n",
      "39 activation_1197\n",
      "40 mixed0\n",
      "41 conv2d_956\n",
      "42 batch_normalization_956\n",
      "43 activation_1201\n",
      "44 conv2d_954\n",
      "45 conv2d_957\n",
      "46 batch_normalization_954\n",
      "47 batch_normalization_957\n",
      "48 activation_1199\n",
      "49 activation_1202\n",
      "50 average_pooling2d_92\n",
      "51 conv2d_953\n",
      "52 conv2d_955\n",
      "53 conv2d_958\n",
      "54 conv2d_959\n",
      "55 batch_normalization_953\n",
      "56 batch_normalization_955\n",
      "57 batch_normalization_958\n",
      "58 batch_normalization_959\n",
      "59 activation_1198\n",
      "60 activation_1200\n",
      "61 activation_1203\n",
      "62 activation_1204\n",
      "63 mixed1\n",
      "64 conv2d_963\n",
      "65 batch_normalization_963\n",
      "66 activation_1208\n",
      "67 conv2d_961\n",
      "68 conv2d_964\n",
      "69 batch_normalization_961\n",
      "70 batch_normalization_964\n",
      "71 activation_1206\n",
      "72 activation_1209\n",
      "73 average_pooling2d_93\n",
      "74 conv2d_960\n",
      "75 conv2d_962\n",
      "76 conv2d_965\n",
      "77 conv2d_966\n",
      "78 batch_normalization_960\n",
      "79 batch_normalization_962\n",
      "80 batch_normalization_965\n",
      "81 batch_normalization_966\n",
      "82 activation_1205\n",
      "83 activation_1207\n",
      "84 activation_1210\n",
      "85 activation_1211\n",
      "86 mixed2\n",
      "87 conv2d_968\n",
      "88 batch_normalization_968\n",
      "89 activation_1213\n",
      "90 conv2d_969\n",
      "91 batch_normalization_969\n",
      "92 activation_1214\n",
      "93 conv2d_967\n",
      "94 conv2d_970\n",
      "95 batch_normalization_967\n",
      "96 batch_normalization_970\n",
      "97 activation_1212\n",
      "98 activation_1215\n",
      "99 max_pooling2d_48\n",
      "100 mixed3\n",
      "101 conv2d_975\n",
      "102 batch_normalization_975\n",
      "103 activation_1220\n",
      "104 conv2d_976\n",
      "105 batch_normalization_976\n",
      "106 activation_1221\n",
      "107 conv2d_972\n",
      "108 conv2d_977\n",
      "109 batch_normalization_972\n",
      "110 batch_normalization_977\n",
      "111 activation_1217\n",
      "112 activation_1222\n",
      "113 conv2d_973\n",
      "114 conv2d_978\n",
      "115 batch_normalization_973\n",
      "116 batch_normalization_978\n",
      "117 activation_1218\n",
      "118 activation_1223\n",
      "119 average_pooling2d_94\n",
      "120 conv2d_971\n",
      "121 conv2d_974\n",
      "122 conv2d_979\n",
      "123 conv2d_980\n",
      "124 batch_normalization_971\n",
      "125 batch_normalization_974\n",
      "126 batch_normalization_979\n",
      "127 batch_normalization_980\n",
      "128 activation_1216\n",
      "129 activation_1219\n",
      "130 activation_1224\n",
      "131 activation_1225\n",
      "132 mixed4\n",
      "133 conv2d_985\n",
      "134 batch_normalization_985\n",
      "135 activation_1230\n",
      "136 conv2d_986\n",
      "137 batch_normalization_986\n",
      "138 activation_1231\n",
      "139 conv2d_982\n",
      "140 conv2d_987\n",
      "141 batch_normalization_982\n",
      "142 batch_normalization_987\n",
      "143 activation_1227\n",
      "144 activation_1232\n",
      "145 conv2d_983\n",
      "146 conv2d_988\n",
      "147 batch_normalization_983\n",
      "148 batch_normalization_988\n",
      "149 activation_1228\n",
      "150 activation_1233\n",
      "151 average_pooling2d_95\n",
      "152 conv2d_981\n",
      "153 conv2d_984\n",
      "154 conv2d_989\n",
      "155 conv2d_990\n",
      "156 batch_normalization_981\n",
      "157 batch_normalization_984\n",
      "158 batch_normalization_989\n",
      "159 batch_normalization_990\n",
      "160 activation_1226\n",
      "161 activation_1229\n",
      "162 activation_1234\n",
      "163 activation_1235\n",
      "164 mixed5\n",
      "165 conv2d_995\n",
      "166 batch_normalization_995\n",
      "167 activation_1240\n",
      "168 conv2d_996\n",
      "169 batch_normalization_996\n",
      "170 activation_1241\n",
      "171 conv2d_992\n",
      "172 conv2d_997\n",
      "173 batch_normalization_992\n",
      "174 batch_normalization_997\n",
      "175 activation_1237\n",
      "176 activation_1242\n",
      "177 conv2d_993\n",
      "178 conv2d_998\n",
      "179 batch_normalization_993\n",
      "180 batch_normalization_998\n",
      "181 activation_1238\n",
      "182 activation_1243\n",
      "183 average_pooling2d_96\n",
      "184 conv2d_991\n",
      "185 conv2d_994\n",
      "186 conv2d_999\n",
      "187 conv2d_1000\n",
      "188 batch_normalization_991\n",
      "189 batch_normalization_994\n",
      "190 batch_normalization_999\n",
      "191 batch_normalization_1000\n",
      "192 activation_1236\n",
      "193 activation_1239\n",
      "194 activation_1244\n",
      "195 activation_1245\n",
      "196 mixed6\n",
      "197 conv2d_1005\n",
      "198 batch_normalization_1005\n",
      "199 activation_1250\n",
      "200 conv2d_1006\n",
      "201 batch_normalization_1006\n",
      "202 activation_1251\n",
      "203 conv2d_1002\n",
      "204 conv2d_1007\n",
      "205 batch_normalization_1002\n",
      "206 batch_normalization_1007\n",
      "207 activation_1247\n",
      "208 activation_1252\n",
      "209 conv2d_1003\n",
      "210 conv2d_1008\n",
      "211 batch_normalization_1003\n",
      "212 batch_normalization_1008\n",
      "213 activation_1248\n",
      "214 activation_1253\n",
      "215 average_pooling2d_97\n",
      "216 conv2d_1001\n",
      "217 conv2d_1004\n",
      "218 conv2d_1009\n",
      "219 conv2d_1010\n",
      "220 batch_normalization_1001\n",
      "221 batch_normalization_1004\n",
      "222 batch_normalization_1009\n",
      "223 batch_normalization_1010\n",
      "224 activation_1246\n",
      "225 activation_1249\n",
      "226 activation_1254\n",
      "227 activation_1255\n",
      "228 mixed7\n",
      "229 conv2d_1013\n",
      "230 batch_normalization_1013\n",
      "231 activation_1258\n",
      "232 conv2d_1014\n",
      "233 batch_normalization_1014\n",
      "234 activation_1259\n",
      "235 conv2d_1011\n",
      "236 conv2d_1015\n",
      "237 batch_normalization_1011\n",
      "238 batch_normalization_1015\n",
      "239 activation_1256\n",
      "240 activation_1260\n",
      "241 conv2d_1012\n",
      "242 conv2d_1016\n",
      "243 batch_normalization_1012\n",
      "244 batch_normalization_1016\n",
      "245 activation_1257\n",
      "246 activation_1261\n",
      "247 max_pooling2d_49\n",
      "248 mixed8\n",
      "249 conv2d_1021\n",
      "250 batch_normalization_1021\n",
      "251 activation_1266\n",
      "252 conv2d_1018\n",
      "253 conv2d_1022\n",
      "254 batch_normalization_1018\n",
      "255 batch_normalization_1022\n",
      "256 activation_1263\n",
      "257 activation_1267\n",
      "258 conv2d_1019\n",
      "259 conv2d_1020\n",
      "260 conv2d_1023\n",
      "261 conv2d_1024\n",
      "262 average_pooling2d_98\n",
      "263 conv2d_1017\n",
      "264 batch_normalization_1019\n",
      "265 batch_normalization_1020\n",
      "266 batch_normalization_1023\n",
      "267 batch_normalization_1024\n",
      "268 conv2d_1025\n",
      "269 batch_normalization_1017\n",
      "270 activation_1264\n",
      "271 activation_1265\n",
      "272 activation_1268\n",
      "273 activation_1269\n",
      "274 batch_normalization_1025\n",
      "275 activation_1262\n",
      "276 mixed9_0\n",
      "277 concatenate_21\n",
      "278 activation_1270\n",
      "279 mixed9\n",
      "280 conv2d_1030\n",
      "281 batch_normalization_1030\n",
      "282 activation_1275\n",
      "283 conv2d_1027\n",
      "284 conv2d_1031\n",
      "285 batch_normalization_1027\n",
      "286 batch_normalization_1031\n",
      "287 activation_1272\n",
      "288 activation_1276\n",
      "289 conv2d_1028\n",
      "290 conv2d_1029\n",
      "291 conv2d_1032\n",
      "292 conv2d_1033\n",
      "293 average_pooling2d_99\n",
      "294 conv2d_1026\n",
      "295 batch_normalization_1028\n",
      "296 batch_normalization_1029\n",
      "297 batch_normalization_1032\n",
      "298 batch_normalization_1033\n",
      "299 conv2d_1034\n",
      "300 batch_normalization_1026\n",
      "301 activation_1273\n",
      "302 activation_1274\n",
      "303 activation_1277\n",
      "304 activation_1278\n",
      "305 batch_normalization_1034\n",
      "306 activation_1271\n",
      "307 mixed9_1\n",
      "308 concatenate_22\n",
      "309 activation_1279\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1776, in excepthook\n",
      "    self.showtraceback((etype, value, tb), tb_offset=0)\n",
      "KeyboardInterrupt\n",
      "\n",
      "Original exception was:\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "    PyErr_CheckSignals()\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "    PyErr_CheckSignals()\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x0000017ABE014A80> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 963, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='output/logs', histogram_freq=1, write_graph=True, write_images=False)\n",
    "tensorboard.set_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=83,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=25,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 46 - ETA: 34 - ETA: 23 - ETA: 11 - 316s 11s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00385027, 0.00223356, 0.00263116, ..., 0.00190942, 0.02419008,\n",
       "        0.0052945 ],\n",
       "       [0.00362196, 0.00516471, 0.00250237, ..., 0.00208397, 0.01137248,\n",
       "        0.00702833],\n",
       "       [0.00315248, 0.00420116, 0.00483776, ..., 0.00340643, 0.01122256,\n",
       "        0.00459765],\n",
       "       ...,\n",
       "       [0.00493188, 0.00444942, 0.00286971, ..., 0.00207865, 0.01424604,\n",
       "        0.00548674],\n",
       "       [0.00493173, 0.00577004, 0.00434456, ..., 0.00548583, 0.00592475,\n",
       "        0.00651255],\n",
       "       [0.0099298 , 0.00679917, 0.00645528, ..., 0.00247888, 0.00785184,\n",
       "        0.00897504]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
